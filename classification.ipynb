{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTIS24 = './data/PASTIS24/'\n",
    "PASTIS9 = './data/PASTIS9/'\n",
    "\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24) [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.\n",
      "   1.  1.  1.  1.  1.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  3.  3.  3.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  0.  0.  0.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  1.\n",
      "   1.  1.  0. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  1.  1.  1.  1.  0.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3.  3.  3.  3.  0. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [ 3.  3.  3. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10. 10.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10. 10. 10. 10.  0.]\n",
      " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      "  10. 10.  1.  1.  1.  1.]]\n",
      "DOY:  (43,) [ 17  22  47  52  57  72  87 102 112 117 132 147 152 157 172 177 182 187\n",
      " 192 197 212 222 227 232 237 242 247 257 262 267 267 272 277 282 282 292\n",
      " 292 297 302 312 317 322 327]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape, data['labels'])\n",
    "print('DOY: ', data['doy'].shape, data['doy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)[:500]\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dataset))\n",
    "plt.imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "print(label[0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Spatial Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, img_height=9, img_width=9, in_channel=10,\n",
    "                       patch_size=3, embed_dim=512, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Parameters\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_cls_token = nn.Parameter(torch.randn(1, 1, self.d)) # (1, 1, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(self.d), nn.Linear(self.d, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "\n",
    "        # CLS Token\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d) # (B*N, K+T, d)\n",
    "\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "        \n",
    "        # CLS Token\n",
    "        spatial_cls_token = self.spatial_cls_token # (1, 1, d)\n",
    "        spatial_cls_token = spatial_cls_token.repeat(B, self.K, 1) # (B, K, d)\n",
    "        spatial_cls_token = spatial_cls_token.unsqueeze(2) # (B, K, 1, d)\n",
    "\n",
    "        x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, N + 1, d)\n",
    "    \n",
    "        x = x.view(B*(self.N+1), self.K, self.d) # (B*(N+1), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(N+1), K, d)\n",
    "        x = x.view(B, self.N+1, self.K, self.d) # (B, (N+1), K, d)\n",
    "        classes = x[:,0,:,:] # (B, K, d)\n",
    "        # print(classes)\n",
    "        x = self.mlp_head(classes) # (B, K, 1)\n",
    "        # print(x, x.shape)\n",
    "        x = x.reshape(B, self.K) # (B, K)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = [x[0,:-1,:,:].reshape(10, 81) for x, y in data]\n",
    "Y = [y for x, y in data]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Credits to  github.com/clcarwin/focal_loss_pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=8, alpha=torch.ones(20), reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.reduction is None:\n",
    "            return loss\n",
    "        elif self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"FocalLoss: reduction parameter not in list of acceptable values [\\\"mean\\\", \\\"sum\\\", None]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  1128329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoderLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 2\n",
    "dataset = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_samples = dataset.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=8, num_layers=8, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = FocalLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-3, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:42<00:00,  5.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  tensor(150.0118, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:43<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  tensor(149.5231, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:44<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Loss:  tensor(149.3623, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:44<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Loss:  tensor(149.1327, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 78/250 [00:14<00:31,  5.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/main.ipynb Cell 37\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X51sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(dataset):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'./weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine\n",
    "\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    print(batch,)\n",
    "    x, y = batch[0].to(device), batch[0].to(device)\n",
    "    print(x.shape, y.shape)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "val_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(val_evaluator, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(dataset)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score function to return current value of any metric we defined above in val_metrics\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "# Checkpoint to store n_saved best models wrt score function\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer), # helps fetch the trainer's state\n",
    ")\n",
    "  \n",
    "# Save the model after every epoch of val_evaluator is completed\n",
    "val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")\n",
    "\n",
    "# Attach handler to plot trainer's loss every 100 iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=100),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(dataset, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Classification(img_width=9, img_height=9, in_channel=10, patch_size=3, embed_dim=128, max_time=60)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('weights/epoch_60.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataset))\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(model(x), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
