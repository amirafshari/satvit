{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/PASTIS/20105_180.pickle')\n",
    "print(data.keys())\n",
    "print(data['img'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutOrPad(object):\n",
    "    \"\"\"\n",
    "    Pad series with zeros (matching series elements) to a max sequence length or cut sequential parts\n",
    "    items in  : inputs, *inputs_backward, labels\n",
    "    items out : inputs, *inputs_backward, labels, seq_lengths\n",
    "\n",
    "    REMOVE DEEPCOPY OR REPLACE WITH TORCH FUN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len=60, random_sample=False, from_start=False):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.random_sample = random_sample\n",
    "        self.from_start = from_start\n",
    "        assert int(random_sample) * int(from_start) == 0, \"choose either one of random, from start sequence cut methods but not both\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        seq_len = deepcopy(sample['img'].shape[0])\n",
    "        sample['img'] = self.pad_or_cut(sample['img'])\n",
    "        if seq_len > self.max_seq_len:\n",
    "            seq_len = self.max_seq_len\n",
    "        sample['seq_lengths'] = seq_len\n",
    "        return sample\n",
    "\n",
    "    def pad_or_cut(self, tensor, dtype=torch.float32):\n",
    "        seq_len = tensor.shape[0]\n",
    "        diff = self.max_seq_len - seq_len\n",
    "        if diff > 0:\n",
    "            tsize = list(tensor.shape)\n",
    "            if len(tsize) == 1:\n",
    "                pad_shape = [diff]\n",
    "            else:\n",
    "                pad_shape = [diff] + tsize[1:]\n",
    "            tensor = torch.cat((tensor, torch.zeros(pad_shape, dtype=dtype)), dim=0)\n",
    "        elif diff < 0:\n",
    "            if self.random_sample:\n",
    "                return tensor[self.random_subseq(seq_len)]\n",
    "            elif self.from_start:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = torch.randint(seq_len - self.max_seq_len, (1,))[0]\n",
    "            tensor = tensor[start_idx:start_idx+self.max_seq_len]\n",
    "        return tensor\n",
    "    \n",
    "    def random_subseq(self, seq_len):\n",
    "        return torch.randperm(seq_len)[:self.max_seq_len].sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('float32')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy'])\n",
    "\n",
    "        del data['doy']\n",
    "        data = self.to_cutorpad(data)\n",
    "        del data['seq_lengths']\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb(x, batch_index=0, t_show=1):\n",
    "    \"\"\"Utility function to get a displayable rgb image \n",
    "    from a Sentinel-2 time series.\n",
    "    \"\"\"\n",
    "    im = x[t_show, [2,1,0]]\n",
    "    mx = im.max(axis=(1,2))\n",
    "    mi = im.min(axis=(1,2))   \n",
    "    im = (im - mi[:,None,None])/(mx - mi)[:,None,None]\n",
    "    im = im.swapaxes(0,2).swapaxes(0,1)\n",
    "    im = np.clip(im, a_max=1, a_min=0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PASTIS('./data/PASTIS/',)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dataset))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "# axes[1].imshow(label[0].numpy())\n",
    "\n",
    "axes[0].set_title('img')\n",
    "# axes[1].set_title(f'{label.unique()}')\n",
    "print(label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Spatial Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=8, dim_feedforward=2048,\n",
    "                       num_layers=8\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        # xt = (xt * 365.0001).to(torch.int64)\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, 1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d)\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K] # (B, N, K, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "\n",
    "        x = x.view(B*(self.N), self.K, self.d) # (B*(N), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(N), K, d)\n",
    "        x = x.view(B, self.N, self.K, self.d) # (B, (N), K, d)\n",
    "\n",
    "        # classes = x[:,0,:,:] # (B, K, d)\n",
    "\n",
    "        x = x[:,1:,:,:] # (B, N, K, d)\n",
    "\n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        x = x.view(B, self.N, self.K, self.P, self.P) # (B, N, K, P, P)\n",
    "\n",
    "        x = x.view(B, self.K, self.N, self.P*self.P) # (B, K, N, P*P)\n",
    "\n",
    "        x = x.view(B, self.K, H, W) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, img_height=9, img_width=9, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=8, dim_feedforward=2048,\n",
    "                       num_layers=8\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(self.d),nn.Linear(self.d, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d)\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K] # (B, N, K, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "\n",
    "        spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, 1, 1, d)\n",
    "\n",
    "        x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, N + 1, d)\n",
    "    \n",
    "        x = x.view(B*(self.N+1), self.K, self.d) # (B*(N+1), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(N+1), K, d)\n",
    "        x = x.view(B, self.N+1, self.K, self.d) # (B, (N+1), K, d)\n",
    "        classes = x[:,0,:,:] # (B, K, d)\n",
    "        x = self.mlp_head(classes) # (B, K, 1)\n",
    "        x = x.reshape(B, -1) # (B, K)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Credits to  github.com/clcarwin/focal_loss_pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=8, alpha=torch.ones(20), reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.reduction is None:\n",
    "            return loss\n",
    "        elif self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"FocalLoss: reduction parameter not in list of acceptable values [\\\"mean\\\", \\\"sum\\\", None]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = Classification(img_width=9, img_height=9, in_channel=10, patch_size=3, embed_dim=128, max_time=60)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print(num_params)\n",
    "\n",
    "\n",
    "# Loss\n",
    "# criterion = MaskedCrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = FocalLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "epoch = 10000\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "  t1 = time.time()\n",
    "  for img, label in dataset:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device).float()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "\n",
    "    # print(output[0], label[0])\n",
    "    \n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': i,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'./weights/epoch_{i}.pt')\n",
    "  t2 = time.time()\n",
    "  print('Epoch: ', i, 'Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
