{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Spatial Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSViT(nn.Module):\n",
    "    def __init__(self, img_height=128, img_width=128, in_channel=10, patch_size=2, embed_dim=128, max_time=60, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.nhead = 4\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = 4\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        # xt = (xt * 365.0001).to(torch.int64)\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, 1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d)\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K] # (B, N, K, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "\n",
    "        spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        \n",
    "        spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        x = x.view(B*(self.N+1), self.K, self.d) # (B*(1+N), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(1+N), K, d)\n",
    "        x = x.view(B, self.N+1, self.K, self.d) # (B, (1+N), K, d)\n",
    "\n",
    "        classes = x[:,0,:,:] # (B, K, d)\n",
    "\n",
    "        x = x[:,1:,:,:] # (B, N, K, d)\n",
    "\n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        x = x.view(B, self.N, self.K, self.P, self.P)\n",
    "\n",
    "\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSViT(\n",
       "  (encoderLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvit = TSViT(32,32,10,2,128,60)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tsvit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 60, 11, 32, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 60, 10, 32, 32)\n",
    "t = torch.randint(0, 366, (2, 60, 1, 32, 32))\n",
    "x = torch.cat([x, t], dim=2)\n",
    "x = x.to(device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 20, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "tsvit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588804"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in tsvit.parameters() if p.requires_grad == True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
