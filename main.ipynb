{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "(43, 10, 24, 24)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('data/PASTIS/10000_0.pickle')\n",
    "print(data.keys())\n",
    "print(data['img'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutOrPad(object):\n",
    "    \"\"\"\n",
    "    Pad series with zeros (matching series elements) to a max sequence length or cut sequential parts\n",
    "    items in  : inputs, *inputs_backward, labels\n",
    "    items out : inputs, *inputs_backward, labels, seq_lengths\n",
    "\n",
    "    REMOVE DEEPCOPY OR REPLACE WITH TORCH FUN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len=60, random_sample=False, from_start=False):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.random_sample = random_sample\n",
    "        self.from_start = from_start\n",
    "        assert int(random_sample) * int(from_start) == 0, \"choose either one of random, from start sequence cut methods but not both\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        seq_len = deepcopy(sample['img'].shape[0])\n",
    "        sample['img'] = self.pad_or_cut(sample['img'])\n",
    "        if seq_len > self.max_seq_len:\n",
    "            seq_len = self.max_seq_len\n",
    "        sample['seq_lengths'] = seq_len\n",
    "        return sample\n",
    "\n",
    "    def pad_or_cut(self, tensor, dtype=torch.float32):\n",
    "        seq_len = tensor.shape[0]\n",
    "        diff = self.max_seq_len - seq_len\n",
    "        if diff > 0:\n",
    "            tsize = list(tensor.shape)\n",
    "            if len(tsize) == 1:\n",
    "                pad_shape = [diff]\n",
    "            else:\n",
    "                pad_shape = [diff] + tsize[1:]\n",
    "            tensor = torch.cat((tensor, torch.zeros(pad_shape, dtype=dtype)), dim=0)\n",
    "        elif diff < 0:\n",
    "            if self.random_sample:\n",
    "                return tensor[self.random_subseq(seq_len)]\n",
    "            elif self.from_start:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = torch.randint(seq_len - self.max_seq_len, (1,))[0]\n",
    "            tensor = tensor[start_idx:start_idx+self.max_seq_len]\n",
    "        return tensor\n",
    "    \n",
    "    def random_subseq(self, seq_len):\n",
    "        return torch.randperm(seq_len)[:self.max_seq_len].sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('float32')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy'])\n",
    "\n",
    "        del data['doy']\n",
    "        data = self.to_cutorpad(data)\n",
    "        del data['seq_lengths']\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PASTIS('./data/PASTIS/',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 60, 11, 24, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = next(iter(dataset))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Spatial Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSViT(nn.Module):\n",
    "    def __init__(self, max_time=60, in_channel=10, img_height=24, img_width=24, patch_size=2, embed_dim=128, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.nhead = 4\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = 4\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        # xt = (xt * 365.0001).to(torch.int64)\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, 1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d)\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K] # (B, N, K, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "\n",
    "        spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        \n",
    "        spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        x = x.view(B*(self.N+1), self.K, self.d) # (B*(1+N), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(1+N), K, d)\n",
    "        x = x.view(B, self.N+1, self.K, self.d) # (B, (1+N), K, d)\n",
    "\n",
    "        classes = x[:,0,:,:] # (B, K, d)\n",
    "\n",
    "        x = x[:,1:,:,:] # (B, N, K, d)\n",
    "\n",
    "        x = x.reshape(-1, self.d)\n",
    "\n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        x = x.view(B, self.N, self.K, self.P, self.P) # (B, N, K, P, P)\n",
    "\n",
    "        x = x.view(B, self.K, self.N, self.P*self.P) # (B, K, N, P*P)\n",
    "\n",
    "        x = x.view(B, self.K, H, W) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSViT(\n",
       "  (encoderLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TSViT(img_width=24,img_height=24,in_channel=10,patch_size=2, embed_dim=128,max_time=60)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(2, 60, 10, 32, 32)\n",
    "# t = torch.randint(0, 366, (2, 60, 1, 32, 32))\n",
    "# x = torch.cat([x, t], dim=2)\n",
    "# x = x.to(device)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time\n",
    "from torch.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n",
      "torch.Size([1, 24, 24]) torch.Size([1, 24, 24, 20])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/main.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, label \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "  t1 = time.time()\n",
    "  for img, label in dataset:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img).permute(0,2,3,1)\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  t2 = time.time()\n",
    "  print(label.shape, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
