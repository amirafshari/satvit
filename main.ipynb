{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "(43, 10, 24, 24)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('data/PASTIS/10000_0.pickle')\n",
    "print(data.keys())\n",
    "print(data['img'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutOrPad(object):\n",
    "    \"\"\"\n",
    "    Pad series with zeros (matching series elements) to a max sequence length or cut sequential parts\n",
    "    items in  : inputs, *inputs_backward, labels\n",
    "    items out : inputs, *inputs_backward, labels, seq_lengths\n",
    "\n",
    "    REMOVE DEEPCOPY OR REPLACE WITH TORCH FUN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_len=60, random_sample=False, from_start=False):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.random_sample = random_sample\n",
    "        self.from_start = from_start\n",
    "        assert int(random_sample) * int(from_start) == 0, \"choose either one of random, from start sequence cut methods but not both\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        seq_len = deepcopy(sample['img'].shape[0])\n",
    "        sample['img'] = self.pad_or_cut(sample['img'])\n",
    "        if seq_len > self.max_seq_len:\n",
    "            seq_len = self.max_seq_len\n",
    "        sample['seq_lengths'] = seq_len\n",
    "        return sample\n",
    "\n",
    "    def pad_or_cut(self, tensor, dtype=torch.float32):\n",
    "        seq_len = tensor.shape[0]\n",
    "        diff = self.max_seq_len - seq_len\n",
    "        if diff > 0:\n",
    "            tsize = list(tensor.shape)\n",
    "            if len(tsize) == 1:\n",
    "                pad_shape = [diff]\n",
    "            else:\n",
    "                pad_shape = [diff] + tsize[1:]\n",
    "            tensor = torch.cat((tensor, torch.zeros(pad_shape, dtype=dtype)), dim=0)\n",
    "        elif diff < 0:\n",
    "            if self.random_sample:\n",
    "                return tensor[self.random_subseq(seq_len)]\n",
    "            elif self.from_start:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = torch.randint(seq_len - self.max_seq_len, (1,))[0]\n",
    "            tensor = tensor[start_idx:start_idx+self.max_seq_len]\n",
    "        return tensor\n",
    "    \n",
    "    def random_subseq(self, seq_len):\n",
    "        return torch.randperm(seq_len)[:self.max_seq_len].sort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('float32')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy'])\n",
    "\n",
    "        del data['doy']\n",
    "        data = self.to_cutorpad(data)\n",
    "        del data['seq_lengths']\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rgb(x, batch_index=0, t_show=1):\n",
    "    \"\"\"Utility function to get a displayable rgb image \n",
    "    from a Sentinel-2 time series.\n",
    "    \"\"\"\n",
    "    im = x[t_show, [2,1,0]]\n",
    "    mx = im.max(axis=(1,2))\n",
    "    mi = im.min(axis=(1,2))   \n",
    "    im = (im - mi[:,None,None])/(mx - mi)[:,None,None]\n",
    "    im = im.swapaxes(0,2).swapaxes(0,1)\n",
    "    im = np.clip(im, a_max=1, a_min=0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PASTIS('./data/PASTIS/',)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0.,  1.,  2.,  3., 12., 13.])')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(dataset))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy())\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Spatial Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSViT(nn.Module):\n",
    "    def __init__(self, max_time=60, in_channel=10, img_height=24, img_width=24, patch_size=2, embed_dim=128, num_classes=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.nhead = 4\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = 4\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        # self.n = int(self.N**0.5)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.nhead, dim_feedforward=self.dim_feedforward)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        Tekenization\n",
    "        '''\n",
    "        # remove the timestamps (last channel) from the input\n",
    "        x_sits = x[:, :, :-1]\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        \n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.view(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "\n",
    "        # Spatial Encoding (Positional Embeddings)\n",
    "        # we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N) \n",
    "\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "        '''\n",
    "        # in the last channel lies the timestamp\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W)\n",
    "        # convert to one-hot\n",
    "        # xt = (xt * 365.0001).to(torch.int64)\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, 1, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, self.N, 1, 1) # (B, N, K, d)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=2) # (B, N, K+T, d)\n",
    "        x = x.view(B*self.N, self.K + T, self.d)\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.view(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K] # (B, N, K, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        Ps = Ps.unsqueeze(1) # (1, 1, N, d)\n",
    "        x = x + Ps # (B, K, N, d)\n",
    "\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "\n",
    "        x = x.view(B*(self.N), self.K, self.d) # (B*(N), K, d)\n",
    "        x = self.spatial_transformer(x) # (B*(N), K, d)\n",
    "        x = x.view(B, self.N, self.K, self.d) # (B, (N), K, d)\n",
    "\n",
    "        classes = x[:,0,:,:] # (B, K, d)\n",
    "\n",
    "        # x = x[:,1:,:,:] # (B, N, K, d)\n",
    "\n",
    "        x = x.reshape(-1, self.d)\n",
    "\n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        x = x.view(B, self.N, self.K, self.P, self.P) # (B, N, K, P, P)\n",
    "\n",
    "        x = x.view(B, self.K, self.N, self.P*self.P) # (B, K, N, P*P)\n",
    "\n",
    "        x = x.view(B, self.K, H, W) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSViT(\n",
       "  (encoderLayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TSViT(img_width=24,img_height=24,in_channel=10,patch_size=2, embed_dim=128,max_time=60)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(2, 60, 10, 32, 32)\n",
    "# t = torch.randint(0, 366, (2, 60, 1, 32, 32))\n",
    "# x = torch.cat([x, t], dim=2)\n",
    "# x = x.to(device)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571908"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.2880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1 tensor(2.3576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2 tensor(2.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3 tensor(2.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4 tensor(2.3236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5 tensor(2.2449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6 tensor(2.2446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7 tensor(2.2877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8 tensor(2.2291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9 tensor(2.3247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10 tensor(2.2714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11 tensor(2.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12 tensor(2.3303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13 tensor(2.3228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14 tensor(2.2447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "15 tensor(2.2501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "16 tensor(2.2498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "17 tensor(2.2334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "18 tensor(2.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "19 tensor(2.2647, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/main.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m img, label \u001b[39min\u001b[39;00m dataset:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/main.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 10000\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "for i in range(epoch):\n",
    "  t1 = time.time()\n",
    "  for img, label in dataset:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img).permute(0,2,3,1)\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  t2 = time.time()\n",
    "  print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
