{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTIS24 = './data/PASTIS24/'\n",
    "PASTIS9 = './data/PASTIS9/'\n",
    "\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24) [[3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 0. 3. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 3. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3. 3. 3. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 0. 3. 3. 3. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "DOY:  (43,) [ 17  22  47  52  57  72  87 102 112 117 132 147 152 157 172 177 182 187\n",
      " 192 197 212 222 227 232 237 242 247 257 262 267 267 272 277 282 282 292\n",
      " 292 297 302 312 317 322 327]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape, data['labels'])\n",
    "print('DOY: ', data['doy'].shape, data['doy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)[:500]\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0,  1,  2,  3,  4,  5,  9, 14, 17, 19])')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(dataset))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        # self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        # input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSat Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        '''\n",
    "        # Transformer Encoder\n",
    "\n",
    "        # PyTorch Encoder\n",
    "        # self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        # self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        # DeepSat Encoder\n",
    "        self.encoder = Transformer(self.d, self.num_layers, self.num_head, 32, self.d*4)\n",
    "\n",
    "\n",
    "        # torchvision Encoder\n",
    "        # self.encoder = Encoder(seq_length=self.N, num_heads=4, num_layers=4, hidden_dim=self.d, mlp_dim=self.d*4, dropout=0., attention_dropout=0.)\n",
    "\n",
    "\n",
    "        # Patches\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.N, self.K, self.d)) # (N, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Tekenization\n",
    "\n",
    "        Convert the images to a sequence of patches\n",
    "        '''\n",
    "        x_sits = x[:, :, :-1, :, :] # (B, T, C, H, W) -- > Exclude DOY Channel\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.reshape(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N)  we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "\n",
    "        (DOY -> One-Hot -> Projection)\n",
    "        '''\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W) in the last channel lies the DOY feature\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d) (DOY, one-hot encoded to represent the DOY feature and then encoded to d dimensions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "\n",
    "        add temporal embeddings (N*K) to the Time Series patches (T)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, 1, 1, 1) # (B, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.reshape(B*self.N, self.K, self.d) # (B*N, K, d)\n",
    "        x = x.reshape(B*self.N, T, self.d) # (B*N, T, d)\n",
    "        # Temporal Tokens (N*K)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=1) # (B*N, K+T, d)\n",
    "        # Temporal Transformer\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.reshape(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "        x = x.reshape(B*(self.K), self.N, self.d) # (B*K, N, d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        x = x + Ps # (B*K, N, d)\n",
    "        '''\n",
    "        # For Classification Only\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        '''\n",
    "        x = self.spatial_transformer(x) # (B*K, N, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, N, K, d)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Segmentation Head\n",
    "        '''\n",
    "        # classes = x[:,:,0,:] # (B, K, d)\n",
    "        # x = x[:,:,1:,:] # (B, K, N, d)\n",
    "        \n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Reassemble\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1) # (B, N, P*P, K)\n",
    "        x = x.reshape(B, self.N, self.P, self.P, self.K) # (B, N, P, P, K)\n",
    "        x = x.reshape(B, self.H, self.W, self.K) # (B, H, W, K)\n",
    "        # x = x.permute(0, 3, 1, 2) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  2339721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoder): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (spatial_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 8\n",
    "dataset = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_samples = dataset.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=8, num_layers=8, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(dataset):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'./weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dataset))\n",
    "output = torch.argmax(model(img.to(device)), dim=3)\n",
    "\n",
    "fix, axes = plt.subplots(1,3, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "axes[2].imshow(output[0].cpu().numpy(), cmap='inferno')\n",
    "\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'Label: {label[0].unique().tolist()}')\n",
    "axes[2].set_title(f'Prediction: {output[0].cpu().unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
