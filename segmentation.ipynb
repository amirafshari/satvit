{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTIS24 = './data/PASTIS24/'\n",
    "PASTIS9 = './data/PASTIS9/'\n",
    "\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24) [[19.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [19.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [19.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.\n",
      "   3.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.\n",
      "   3.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.\n",
      "   3.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  3.  3.  3.  3.\n",
      "   3.  3.  0.  0.  2.  2.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  0.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  0.  2.  2.]\n",
      " [ 1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  0.  2.  2.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  2.  2.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  2.  2.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  0.  2.]\n",
      " [ 1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  5.  5.  5.  5.\n",
      "   5.  5.  5.  5.  0.  2.]]\n",
      "DOY:  (43,) [ 17  22  47  52  57  72  87 102 112 117 132 147 152 157 172 177 182 187\n",
      " 192 197 212 222 227 232 237 242 247 257 262 267 267 272 277 282 282 292\n",
      " 292 297 302 312 317 322 327]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape, data['labels'])\n",
    "print('DOY: ', data['doy'].shape, data['doy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)[:500]\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0,  1,  2,  3,  4,  5,  6,  9, 10, 12, 17, 19])')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1EAAAGiCAYAAAAGHEw0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCqklEQVR4nO3deXxU9b3/8ffs2QORJUR2pSAgUBAQRUWlIhdRRAVcKlCqVXFB61J7VcRacXlc61VRemsLLuCCFdeWW0XADaniVhURuEFBSWRLQvZZvr8//GVqSEL4fjPJBHk9H495KGfO53y+c+bMfPOe5YzHGGMEAAAAANgv3mQPAAAAAAAOJIQoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QooAELFy6Ux+PR5s2bkz0UAAepu+++W3369FEsFpMkrVy5Uh6PJ355//33kzxCAGj9JkyYEH/e7N+/f3z5559/Lr/fr08//dR6m4QoAMAB7Z133tGtt96qoqKiZA8loUpKSnTXXXfphhtukNdbe7r+7W9/q8cff1w9e/ZsdDuxWEx33323evTooZSUFA0YMEBPPvlkwsf7j3/8QzNmzFD//v3l8/nUvXv3hPeQpPfee0+XX365+vXrp/T0dHXt2lWTJk3Sl19+mfBed9xxh44++mi1b99eKSkp6tWrl2bNmqXt27cnvNcP/f73v6/zx16idO/evVYQr7lccsklCe8lSU8//bRGjBih9PR0tWnTRsccc4xef/31hPbYuHGjzj77bLVt21ZpaWkaOXKkVqxY0aRtPvzwwzrnnHPUtWtXeTweTZs2rcF1i4qKdPHFF6t9+/ZKT0/XiSeeqA8++MC5d2lpqWbPnq1TTz1VOTk58ng8WrhwYZ31YrGYFi5cqNNPP11dunRRenq6+vfvr9tvv12VlZXO/bdt26bf/OY3OvHEE5WZmSmPx6OVK1fWWW/z5s31Hks1l4suusip/z//+U9ddtllGjJkiAKBgDweT4PrFhYWavr06erQoYNSU1M1ePBgLVmypM56V199tR5//HH16dOn1vK+fftq3LhxuuWWW6zH6beuAA4SP//5zzVlyhSFQqFkDwXAPrzzzjuaM2eOpk2bpjZt2iR7OAnzl7/8RZFIROeee26d6372s59p1KhR+7Wd//zP/9Sdd96piy66SEOHDtULL7yg8847Tx6PR1OmTEnYeBcvXqynn35agwcPVl5eXsK2u7e77rpLb7/9ts455xwNGDBABQUFevDBBzV48GC9++67CQ0ea9eu1aBBgzRlyhRlZmZq3bp1+tOf/qRXXnlFH330kdLT0xPWq8bWrVt1xx13NMu2awwaNEi//vWvay37yU9+kvA+t956q2677TadffbZmjZtmsLhsD799FN98803CeuxZcsWjRgxQj6fT9ddd53S09O1YMECnXLKKVq+fLmOP/54p+3edddd2rNnj4YNG6Zt27Y1uF4sFtO4ceP08ccf67rrrlO7du300EMPadSoUVq7dq169epl3XvHjh267bbb1LVrVw0cOLDeACNJ5eXlmj59uo4++mhdcskl6tChg1avXq3Zs2dr+fLlev311/cZQBqyfv163XXXXerVq5eOPPJIrV69ut712rdvr8cff7zO8mXLlmnRokU65ZRTrHtL0t/+9jc98sgjGjBggHr27NngCyQlJSUaOXKkCgsLddVVVyk3N1fPPPOMJk2apEWLFum8886Lr3vCCSdIkh555BHt2LGj1nYuueQS/cd//Ic2bdqkww47bP8HagAAOIDdc889RpLJz89P9lCarLS0NP7/AwYMMBdccEGt61esWGEkmRUrVuzX9rZu3WoCgYCZOXNmfFksFjPHHXec6dy5s4lEIgkZtzHGfPPNN6a6utoYY8y4ceNMt27dErbtH3r77bdNVVVVrWVffvmlCYVC5vzzz2+Wnj/07LPPGknmySefbJbtT5482Zx00knmhBNOMP369Uv49rt162bGjRuX8O3ubfXq1cbj8Zh77723Wftcdtllxu/3my+++CK+rKyszHTp0sUMHjzYebubN282sVjMGGNMenq6mTp1ar3rPf3000aSWbJkSXzZd999Z9q0aWPOPfdcp96VlZVm27Ztxhhj3nvvPSPJLFiwoM56VVVV5u23366zfM6cOUaSefXVV536l5SUmJ07dxpjjFmyZInVc44xxpx88skmKyvLVFRUOPUvKCgw5eXlxhhjZs6caRqKK3fffbeRZJYvXx5fFo1GzdChQ01ubm6d5wljTL2Pq+rqatO2bVtz8803W42Tj/MBDdj7O1Hdu3fXaaedppUrV+qoo45SamqqjjzyyPgrRM8995yOPPJIpaSkaMiQIfrwww/rbHPJkiXq27evUlJS1L9/fy1dulTTpk1rto+9AD92t956q6677jpJUo8ePeIfI/nhdxmfeOIJDRkyRKmpqcrJydGUKVO0ZcuWWtsZNWqU+vfvr88//1wnnnii0tLSdOihh+ruu++u0/OBBx5Qv379lJaWprZt2+qoo47S4sWLa63z4YcfauzYscrKylJGRoZOPvlkvfvuu7XWqXmOWbVqlS677DJ16NBBnTt3liTl5+frk08+0ejRo5u0f1544QWFw2Fddtll8WUej0eXXnqptm7d2uArzC7y8vIUCAQStr2GHHPMMQoGg7WW9erVS/369dO6deuavX/N83VzfHz0jTfe0LPPPqv77rsv4dveW3V1tcrKyppt+/fdd59yc3N11VVXyRij0tLSZunz5ptv6qc//al69+4dX5aWlqbTTz9dH3zwgTZs2OC03W7duu3XuzjPPvusOnbsqIkTJ8aXtW/fXpMmTdILL7ygqqoq696hUEi5ubmNrhcMBnXMMcfUWX7mmWdKkvPjITMzUzk5OU6127Zt04oVKzRx4kSlpKQ4baNjx45KTU1tdL0333xT7du310knnRRf5vV6NWnSJBUUFGjVqlX71S8QCGjUqFF64YUXrMZJiAIsbNy4Ueedd57Gjx+vuXPnavfu3Ro/frwWLVqkq6++WhdccIHmzJmjTZs2adKkSfEvg0vSK6+8osmTJysQCGju3LmaOHGiZsyYobVr1ybxFgEHtokTJ8Y/7vaHP/xBjz/+uB5//HG1b99e0vffLbnwwgvVq1cv3XvvvZo1a1b8Iz57/xG8e/dunXrqqRo4cKD+67/+S3369NENN9ygv//97/F1/vSnP+nKK69U3759dd9992nOnDkaNGiQ1qxZE1/ns88+03HHHaePP/5Y119/vW6++Wbl5+dr1KhRtdarcdlll+nzzz/XLbfcot/85jeSvv+IoiQNHjy4Sfvnww8/VHp6uo444ohay4cNGxa//sfAGKPCwkK1a9euWba9Y8cOFRQU6M0339SVV14pn8+33x+n3F/RaFRXXHGFfvnLX+rII49M6Lb39vrrrystLU0ZGRnq3r27/vu//zvhPZYvX66hQ4fq/vvvV/v27ZWZmalOnTrpwQcfTGifqqqqev/gTktLk6Rmn2M//PBDDR48uM73FocNG6by8vJm+a5eYwoKCiSpWR4PjXnqqacUi8V0/vnnN3uvRN73Q4YM0aeffqqSkpL9ruE7UYCF9evX65133tGIESMkff+FxDFjxuiiiy7SF198oa5du0qS2rZtq1/96ld644034hPtjTfeqEMPPVRvv/22MjIyJEknn3yyRo0apW7duiXl9gAHugEDBmjw4MF68sknNWHChFrv6n711VeaPXu2br/9dv32t7+NL584caJ++tOf6qGHHqq1/Ntvv9Vjjz2mn//855KkGTNmqFu3bvrzn/+ssWPHSvr+xZB+/frV+8XlGjfddJPC4bDeeuut+IkfLrzwQvXu3VvXX399nVdHc3JytHz5cvl8vviyL774QtL37641xbZt29SxY8c6r6h36tQpfpt/DBYtWqRvvvlGt912W8K3XVhYGN9fktS5c2ctXry4zhfUm2r+/Pn66quv9NprryV0u3sbMGCARo4cqd69e2vnzp1auHChZs2apW+//VZ33XVXQnrs3r1bO3bs0Ntvv63XX39ds2fPVteuXbVgwQJdccUVCgQC+tWvfpWQXr1799abb76pPXv2KDMzM778rbfekqSEfv+qPtu2bav3e1c/fIw1dyje2913362srKz481ZLWrRokTp16lTr3aHm0rt3b7322mv66quvav0d9eabb0qyu+979uypWCymL774Iv4iU2N4Jwqw0Ldv33iAkqThw4dLkk466aR4gPrh8v/7v/+T9P2T6L/+9S9deOGF8QAlff9Fx5Z+cgUOFs8995xisZgmTZqkHTt2xC+5ubnq1atXnbN3ZWRk6IILLoj/OxgMatiwYfHHsSS1adNGW7du1XvvvVdvz2g0qn/84x+aMGFCrTPnderUSeedd57eeuutOq90XnTRRbUClCTt3LlTfr+/1vOFi4qKinpPjlPzMZuKioombb81+OKLLzRz5kyNGDFCU6dOTfj2c3Jy9Oqrr+qll17Sbbfdpnbt2iX8o2k7d+7ULbfcoptvvjn+LmpzefHFF3X99dfrjDPO0C9+8QutWrVKY8aM0b333qutW7cmpEfN/tm5c6ceeeQRXXvttZo0aZJeeeUV9e3bV7fffntC+kjSpZdeqqKiIk2ePFkffvihvvzyS82aNSt++v/mPsZb22Psjjvu0GuvvaY777yzxU+08+WXX2rt2rWaMmVKnXfmmsMvf/lL+Xw+TZo0Se+88442bdqkuXPnaunSpZLs9n3btm0lqc5JJ/aFEAVY+GFQkqTs7GxJUpcuXepdvnv3bknfvyIuSYcffnidbda3DEDTbdiwQcYY9erVS+3bt691Wbdunb777rta63fu3LnOOzZt27aNP44l6YYbblBGRoaGDRumXr16aebMmXr77bfj12/fvl3l5eW1vp9R44gjjlAsFqvzfaymvtu0L6mpqfV+J6Pm9Mf7872D1qygoEDjxo1Tdna2nn322TphNBGCwaBGjx6t0047TTfffLPmzZunGTNm6OWXX05Yj5tuukk5OTm64oorErbN/eXxeHT11VcrEok0eBY4WzXHVSAQ0Nlnnx1f7vV6NXnyZG3dulVff/11QnqNHTtWDzzwgN544w0NHjxYvXv31iuvvKLf//73ktTkFyIa05oeY08//bRuuukmzZgxQ5deemmL9a2xaNEiSWqRj/JJ37+runjxYm3atEnHHnusDj/8cN1///3x7xTa3PfGGEmyOpshH+cDLDQ0QTe0vOZBCaDlxWIxeTwe/f3vf6/3Mbr3BLs/j+MjjjhC69ev18svv6xly5bpr3/9qx566CHdcsstmjNnjtM46/sj65BDDlEkEqnzESVbnTp10ooVK2SMqfXHQc0pm5vzVOTNrbi4WGPHjlVRUZHefPPNFrstxxxzjDp16qRFixbptNNOa/L2NmzYoP/5n//RfffdV+vjlZWVlQqHw9q8ebOysrKcv+i/P2peCNy1a1dCtpeTk6OUlBS1adOmzuOqQ4cOkr5/kXHvFyZdXX755Zo+fbo++eQTBYNBDRo0SH/+858lNc+p23+oU6dO9Z4CvaUfY6+++qouvPBCjRs3TvPnz2+RnntbvHixevfurSFDhrRYz7PPPlunn366Pv74Y0WjUQ0ePDj+YoDNfV/zYpnN98gIUUALqPms7saNG+tcV98yAPuvoVcODzvsMBlj1KNHj4T+IZWenq7Jkydr8uTJqq6u1sSJE/X73/9eN954o9q3b6+0tDStX7++Tt0XX3whr9db553r+tR83yY/P18DBgxwHuugQYP0yCOPaN26derbt298ec0JLgYNGuS87WSqrKzU+PHj9eWXX+q1116rddtaqn9xcXFCtvXNN98oFovpyiuv1JVXXlnn+h49euiqq65q1jP21XxkNVEfJfR6vRo0aJDee+89VVdX1zqbYk1QTPTHFtPT02t93P61115Tamqqjj322IT22dugQYP05ptvKhaL1foI25o1a5SWltbsIa6m15lnnqmjjjpKzzzzjPz+lv/zfs2aNdq4cWOzfC+xMcFgUEOHDo3/u+Z7hTZnN83Pz5fX67W6v/g4H9AC8vLy1L9/fz322GO1Pku/atUq/etf/0riyIADX82Pku59tr2JEyfK5/Npzpw5dd4VNsZo586d1r32rgkGg+rbt6+MMQqHw/L5fDrllFP0wgsv1DrNemFhoRYvXqyRI0cqKyur0T41fwzWfK/D1RlnnKFAIKCHHnoovswYo/nz5+vQQw+t9/TIrV00GtXkyZO1evVqLVmypNYfzolUVlam8vLyOsv/+te/avfu3TrqqKMS0qfm5y72vvTr109du3bV0qVLNWPGjIT02rVrl6LRaK1l4XBYd955p4LBoE488cSE9JGkyZMnKxqN6tFHH40vq6ys1KJFi9S3b99mfYfmnXfe0XPPPacZM2bEP17fXM4++2wVFhbqueeeiy/bsWOHlixZovHjx9f7falEWrduncaNG6fu3bvr5ZdfTtpHdGt+5uGHP3CbDBs2bND8+fN12mmnWQWitWvXql+/flbHC+9EAS3kjjvu0BlnnKFjjz1W06dP1+7du/Xggw+qf//+zfb7GcDBoOajI//5n/+pKVOmKBAIaPz48TrssMN0++2368Ybb9TmzZs1YcIEZWZmKj8/X0uXLtXFF1+sa6+91qrXKaecotzcXB177LHq2LGj1q1bpwcffFDjxo2Lf+zu9ttv16uvvqqRI0fqsssuk9/v1x//+EdVVVXV+7tT9enZs6f69++v1157Tb/4xS/sdsgPdO7cWbNmzdI999yjcDisoUOH6vnnn9ebb76pRYsW1fqo1cKFCzV9+nQtWLBA06ZNs+71ySef6MUXX5T0/TvsxcXF8RMIDBw4UOPHj4+vW3MWxR8Gzf3161//Wi+++KLGjx+vXbt26Yknnqh1/Q9PDtKU27RhwwaNHj1akydPVp8+feT1evX+++/riSeeUPfu3XXVVVfVWt/1NrVr104TJkyos7zmnae9r7v11ls1Z84crVixwvo06y+++KJuv/12nX322erRo4d27dqlxYsX69NPP9Udd9xR67eJNm/erB49emjq1KlauHChVR9J+tWvfqVHHnlEM2fO1JdffqmuXbvq8ccf11dffaWXXnqp1rqjRo3SqlWrnD4C/9VXX2nSpEk6/fTTlZubq88++0zz58/XgAEDdMcdd9Ra1+Z4eOmll/Txxx9L+j5ofvLJJ/Hj+fTTT4+/Q3z22Wfr6KOP1vTp0/X555+rXbt2euihhxSNRut8xHfatGl69NFHlZ+f3+jvQz744IMqKiqKv3P30ksvxU/8ccUVVyg7O1t79uzRmDFjtHv3bl133XV65ZVXam3jsMMOq/Uig81+rrmtn332mSTp8ccfj5/x8Kabbqq1bjQa1dNPP62jjz5ahx12WIPb9Hg8OuGEExr97t1XX32lxx9/XNK/X0iqGU+3bt3iZ1CVvj/h1znnnKOuXbsqPz9fDz/8sHJycqw+0hgOh+O/12fF6qd5gYPIggULjCSTn59vjGn4V94lmZkzZ9Zalp+fbySZe+65p9byp556yvTp08eEQiHTv39/8+KLL5qzzjrL9OnTp9luB3Aw+N3vfmcOPfRQ4/V6az1ujTHmr3/9qxk5cqRJT0836enppk+fPmbmzJlm/fr18XXq+xV7Y4yZOnWq6datW/zff/zjH83xxx9vDjnkEBMKhcxhhx1mrrvuOlNcXFyr7oMPPjBjxowxGRkZJi0tzZx44onmnXfeqbVOzXPMe++9V+9tuvfee01GRoYpLy+PL1uxYoWRZFasWLHf+yYajZo77rjDdOvWzQSDQdOvXz/zxBNP1FnvgQceMJLMsmXL9nvbP1Rze+q7TJ06tda67dq1M0cffbRTnxNOOKHBPnv/WdOU27R9+3Zz8cUXmz59+pj09HQTDAZNr169zKxZs8z27dvrrN+U21Sfho7JX//618bj8Zh169ZZb/P9998348ePN4ceeqgJBoMmIyPDjBw50jzzzDN11v3Xv/5lJJnf/OY3TuM3xpjCwkIzdepUk5OTY0KhkBk+fHi998WQIUNMbm6uU49du3aZM844w+Tm5ppgMGh69OhhbrjhBlNSUlJnXZvjYerUqQ0eYwsWLKgzhhkzZphDDjnEpKWlmRNOOKHex/VZZ51lUlNTze7duxvt361btwb71zy/1fytsb+PO5v9vL+PMWOMWbZsmZFk7r///ga3t2fPHiPJTJkypdHeNc9z9V1OOOGEWutOmTLFdOnSxQSDQZOXl2cuueQSU1hY2OC263tc/f3vfzeSzIYNGxod2w8RooAkGzhwoBk9enSyhwGglSkqKjI5OTnmkUceiS+r+ePi+eefN9u3bzfhcDhh/c455xwzdOjQhG2vIZ999pmRZF5++eVm7/VjvE1Dhw41Z599drP3mTdvnklPTzcFBQXN2qekpMT4/X7z4IMPNmsfY1rueGhIhw4dzLXXXpuU3i25n+vzyiuvGI/HYz755JOk9C8pKTHbt283xxxzTJ0QdcYZZ5gJEyZYb5PvRAEtJBwOKxKJ1Fq2cuVKffzxx9YfyQDw45edna3rr79e99xzj2KxWK3rJkyYoPbt2+ujjz5KSC9jjFauXJnQ3+9pyIoVKzRixAiNGzeuWfv8GG9TSUmJPv744xb58v6KFSt05ZVXqmPHjs3a54033tChhx6qiy66qFn7tOTxUJ/PPvtMFRUVuuGGG5LSv6X2c0NWrFihKVOmJO23MX/+85+rffv2euedd2otX7dunV5++WX97ne/s96mxxjOwQy0hM2bN2v06NG64IILlJeXpy+++ELz589Xdna2Pv30Ux1yyCHJHiKAVm737t1au3Zt/N/Dhw9v0inQAeBg8Mknn8R/GzAjI0NHH310k7dJiAJaSHFxsS6++GK9/fbb2r59u9LT03XyySfrzjvv3OcXMQEAANC6EKIAAAAAwALfiQIAAAAAC4QoAAAAALDQ6n5sNxaL6dtvv1VmZqY8Hk+yhwMABxVjjPbs2aO8vDx5vbzOVoO5CQCSo7XOS60uRH377bfq0qVLsocBAAe1LVu2qHPnzskeRqvB3AQAydXa5qVWF6JqTtW6fP79Sk9Nta6v8uU49Q216+RUJ0kZ7dKda3PblTnX+r77xqnum03bnXvu2LnDufb/trmNt2BHuXPPkr1+l8lGIMXtnCtZ1SXOPcMR+2O+hi8QcK6t9LqdXj3Y3r1n2zT3+8ZXVuBUV1K4y7nn9t0pzrVFgSznWk9G0Kku4PD8KUlVVdX64x8XcNrsvdTsj8WD/kNpPvfjHj8exy9v3t+EAvC9kpIKde96Vaubl1pdiKr5mER6aqoy0tKs6wM+t0ATSs9wqpOkzAz32qws94+F+Mrt948klaS5/6FeWe7+h2Raitsfgykh9z+2q5vwtm8g5BaiUuX+B5bP24TagNv+/b445FQWTHEfb2qKz7nWH3XrGw66P+WlNCGkhoLu943HsTYQasLxIPGRtb3U7I80X0DpfkIUpKwstzkYgJvWNi812wcL582bp+7duyslJUXDhw/XP//5z+ZqBQBAo5iXAACJ0iwh6umnn9Y111yj2bNn64MPPtDAgQM1ZsyY+C8FAwDQkpiXAACJ1Cwh6t5779VFF12k6dOnq2/fvpo/f77S0tL0l7/8pTnaAQCwT8xLAIBESniIqq6u1tq1azV69Oh/N/F6NXr0aK1evbrO+lVVVSopKal1AQAgUWznJYm5CQCwbwkPUTt27FA0GlXHjh1rLe/YsaMKCuqeUWvu3LnKzs6OXziFLAAgkWznJYm5CQCwb0n/xaobb7xRxcXF8cuWLVuSPSQAwEGOuQkAsC8JP8V5u3bt5PP5VFhYWGt5YWGhcnNz66wfCoUUCrmdXhkAgMbYzksScxMAYN8S/k5UMBjUkCFDtHz58viyWCym5cuXa8SIEYluBwDAPjEvAQASrVl+bPeaa67R1KlTddRRR2nYsGG67777VFZWpunTpzdHOwAA9ol5CQCQSM0SoiZPnqzt27frlltuUUFBgQYNGqRly5bV+VIvAAAtgXkJAJBIzRKiJOnyyy/X5Zdf3lybBwDACvMSACBRmi1ENVWkdLci0UrrunBoj1O/si1FTnWSVFTUzrl2zy7jXOurdvvdktJwqXPPPWW7nGuLSsNOdWGfx7lnerCtc21Kqtt9k5GZ4tzT4ZCP88Xcj6WIx+2pIOTNcu6ZkRp1rnU9JvwV6c49MwJlzrXRavevn8Z8Aac6n2JOdV7HOgA/Xn7vVOfaSOzRBI4EaD2SfopzAAAAADiQEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAs+JM9gIYYb1TGG7Gui1TZ10hSSWWlU50keWK7nGuLCwPOtX6/221VrMy5557iEudab8y1Msu5Z5UvxbnWp6BTnTeU5txTKVHn0lC42Lk2s8LnVOdL8zj39ISdS2VK3V7/iXlDzj3DMffb6kt3f70q1bHWH85wqzNVTnXAgepna5Y61UU0IbEDacUisUeT0tfvnepUl6zx4uDCO1EAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAWCFEAAAAAYIEQBQAAAAAW/MkeQEO2lpUqLRaxrqsujzr1K6mocqqTJO9O9yxqvCnOtWlt3e6+aEWpc8+qmHOppHKnKk+p+/6tyDbOtR6v2/6NNeGliUBa0LnWUxlwrk2JhJ3qyiP2j9EalZnuj7nsarfxeqvc6iQpmuJ+31QH3O8bf6rbcZgadKur8Lk9hwIHqkjs0WQPAQ1Ixn3j9051ruVYOrjwThQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFf7IH0JBdO3aqIhSyL4xEnfpFKsJOdZJUEstwrvWme5xrq1XtVBdzb6lQRopzrT9S7lbnLXPumVLm/jqBJzPiVBcLuO9gv3GvDaa0c65N95U41YXL05x7VrXLdK7151Q61XXaXercs22Z+3NEZcx9Pykj3aksNdvnVFdW7rZvgWT62ZqlzrURTUjcQHDAi8QebfGefu9U59pkjBff450oAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC/5kD6AhAZ9R0G8c6iJO/SKV5U51khSr8DjXlkdizrXF4ZBTXUa6z7ln1ASda8P+NKc6T0aJc09Pqfsh7o+5jTcUdO+Z4Xd/XSMWijrXmmLHvrEq556V0U7OtWXtOjrVhdruce6pkrBzabbP7bEqSamBHKc6X247p7pgaZlTHZBMkdijyR4C4OxAPH793qlOdQfibW0I70QBAAAAgAVCFAAAAABYIEQBAAAAgIWEh6hbb71VHo+n1qVPnz6JbgMAwH5jbgIAJFKznFiiX79+eu211/7dxN9qz18BADhIMDcBABKlWWYQv9+v3Nzc5tg0AABOmJsAAInSLN+J2rBhg/Ly8tSzZ0+df/75+vrrrxtct6qqSiUlJbUuAAAkGnMTACBREh6ihg8froULF2rZsmV6+OGHlZ+fr+OOO0579tT/+yxz585VdnZ2/NKlS5dEDwkAcJBjbgIAJFLCQ9TYsWN1zjnnaMCAARozZoz+9re/qaioSM8880y96994440qLi6OX7Zs2ZLoIQEADnLMTQCARGr2b9W2adNGP/nJT7Rx48Z6rw+FQgqFQs09DAAA4pibAABN0ey/E1VaWqpNmzapU6dOzd0KAID9wtwEAGiKhIeoa6+9VqtWrdLmzZv1zjvv6Mwzz5TP59O5556b6FYAAOwX5iYAQCIl/ON8W7du1bnnnqudO3eqffv2GjlypN599121b98+0a0AANgvzE0AgERKeIh66qmnErKd9u3bKj01xbouxVPm1C/sq3Kqk6S0ULlzbUGFc6kqq1Kd6mL2u/XfPSPub15GA22d6kyqx7ln0ONzrvUG3L4PUR1yf1iVewLOtVWRXc61FdXGqa7cuH9nxFcZca4t2x51qvP63PdvLOT+sa/sLPfHTVuf2wPW+Ds41Xn8pU51rV2i5iY0n5+tWepcG9GExA0EQKMisUeTPYSka/bvRAEAAADAjwkhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAs+JM9gIa0PSRPGWmp1nUp3hKnfpEU9zzp/a7UuTa8J+ZcW1RZ4VRX7fM594zGmnDIGLd9HI4FnFs2Zbg+b9ipbk9JtXPPck+Kc62nepdzbfUe41QXMW6PN0mK7S5zrt3ly3aqqw6479922e6PG3/HNOdarz/oVJeeGXGq88itDmiqSOzRZA8BAPYb70QBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABYIEQBAAAAgAVCFAAAAABY8Cd7AA0Je0Kq9oSs64w3y6lfRdDnVCdJu0PfOddGI5XOtQ67R5KUbqLOPcsr3A+ZSDTsVOd1v2sUMZ4mFJc6lYWrKpxbloeNe20s4lwrv9t+8np3O7fMqM50ro3tKXOqq0pLde4Z6ZDiXFti3A/iikC2U12Pth2c6qK+NKc6oMbP1ix1qotoQmIHAgDNiHeiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMCCP9kDaEhZlVfy+azrYlVVTv0qK8JOdZJUXm4/zhqRWKZzbczrNuZwtds+kqRAasi51mOKnOpi1SnOPUuNx7nWG6l0qotGi517lhfFnGt3RoLOtd4Ut/s1PeTeMyUj4Fzrl9v96vW6v25UXlriXlvmfhxmdenoVNezr9v+9bTeaQEHiEjs0WQPAQCaHe9EAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWPAnewANKSwoVGpKinVdLFLm1K8q6lQmSQp5Ys61JhZ2ro1Uufb0Off0equda2MRt51cXe54QyXFmnC/lsqtbzTsfp9WVLjf1kjUONcGXZ8KPM4tVdaE2hyP2zEcjVY49yzd/p1zrUI57qWd7J8HJem7SBunutIIr61B+tmapc61EU1I3EAAoJVitgQAAAAAC4QoAAAAALBAiAIAAAAAC9Yh6o033tD48eOVl5cnj8ej559/vtb1xhjdcsst6tSpk1JTUzV69Ght2LAhUeMFAKAW5iUAQEuzDlFlZWUaOHCg5s2bV+/1d999t+6//37Nnz9fa9asUXp6usaMGaPKysomDxYAgL0xLwEAWpr1KbnGjh2rsWPH1nudMUb33XefbrrpJp1xxhmSpMcee0wdO3bU888/rylTpjRttAAA7IV5CQDQ0hL6naj8/HwVFBRo9OjR8WXZ2dkaPny4Vq9eXW9NVVWVSkpKal0AAEgEl3lJYm4CAOxbQkNUQUGBJKljx461lnfs2DF+3d7mzp2r7Ozs+KVLly6JHBIA4CDmMi9JzE0AgH1L+tn5brzxRhUXF8cvW7ZsSfaQAAAHOeYmAMC+JDRE5ebmSpIKCwtrLS8sLIxft7dQKKSsrKxaFwAAEsFlXpKYmwAA+5bQENWjRw/l5uZq+fLl8WUlJSVas2aNRowYkchWAAA0inkJANAcrM/OV1paqo0bN8b/nZ+fr48++kg5OTnq2rWrZs2apdtvv129evVSjx49dPPNNysvL08TJkxI5LgBAJDEvAQAaHnWIer999/XiSeeGP/3NddcI0maOnWqFi5cqOuvv15lZWW6+OKLVVRUpJEjR2rZsmVKSUlJ3KgBAPj/mJcAAC3NOkSNGjVKxpgGr/d4PLrtttt02223NWlgAADsD+YlAEBLsw5RLaXwu21KCYWs67zpbq8sBjwBpzpJ8jbhm2URT8MTf2OqPRGnOr/f/baGq2LOtZ59/JGzL9WqdO4ZjrrtI0kKF1c51ZVXutVJUjjsfjD5m/AVR5/H41TnDYade3qM+31TXen6uCl37unzute2yWjnXJtaUuxU992m3U51ZaX8HhKkSOzRZA8BAFq1pJ/iHAAAAAAOJIQoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC/5kD6Ah1RUReaM+6zpvrNKpn/FUONVJUkXA41zrCzqXStUxp7JoyD07e1TtXBuuCDvVlRe73aeStDvqfr9WVrrtp3Cl2+2UpIpgyLk2Kxh1rpWnyqksplT3nmG341eSwr4yp7oUpTj3DKS6P1jDvlLn2tLqzW6Fu9yel6rL3Md6MJiw9iVJ7s/5tl4dfmaL9QIA7D/eiQIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALDgT/YAGrK9olqhqH1dlqlw6mdMwKlOkqp87rsxEDTOtV6vWwb2R8LOPT2xiHOtqordysr2OLfcU+5cqki1237aUxFz7lnly3au9XcIutd63I4lj3HvWelcKXmiDk8OkhR1P/Yjle6P81hkt3NtNOb23OTPcdtHldWO+/YgkZ3SVx6Pr8X6/WzNUufaSOzRBI4EAPBDvBMFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABgwZ/sATRkV6xcgVjEui5aXunUr7LavleNMrVxrk31uedYvzFOdYF055byN+WIqapwKtuzx61OkiKRmHNttKzIqa46nOXc05dZ5Vzr9WY714ZCIcee7vtX0Sbc1kCGU12sOuzcM1LpXuvxprr39XVwqguUuj0XVpS73y9IvDap/Z1r26Xf41xbVPGpc20k9qhzLQAcKHgnCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwAIhCgAAAAAsEKIAAAAAwII/2QNoSPnuPQoEAtZ1e8IVTv2qY251khQLVzrXpqb6nGsD3qBTXazEvac33TjX+sqjTnXVVc4tFfFXO9fGfG6vMVRUu7824Tch51qZJvT1uN2vgdSYc880j3Op/CluB4XH8XZKkj/iXCpF3R6rkuT3lDvVpUZ2OtWZiFs//Li0Se3vXNsu/R6nuh1l1zn3BICWxjtRAAAAAGCBEAUAAAAAFqxD1BtvvKHx48crLy9PHo9Hzz//fK3rp02bJo/HU+ty6qmnJmq8AADUwrwEAGhp1iGqrKxMAwcO1Lx58xpc59RTT9W2bdvilyeffLJJgwQAoCHMSwCAlmZ9YomxY8dq7Nix+1wnFAopNzfXeVAAAOwv5iUAQEtrlu9ErVy5Uh06dFDv3r116aWXaudOt7NEAQCQCMxLAIBESvgpzk899VRNnDhRPXr00KZNm/Tb3/5WY8eO1erVq+Xz1T21dlVVlaqq/n264pKSkkQPCQBwELOdlyTmJgDAviU8RE2ZMiX+/0ceeaQGDBigww47TCtXrtTJJ59cZ/25c+dqzpw5iR4GAACS7OclibkJALBvzX6K8549e6pdu3bauHFjvdffeOONKi4ujl+2bNnS3EMCABzEGpuXJOYmAMC+JfydqL1t3bpVO3fuVKdOneq9PhQKKRQKNfcwAACQ1Pi8JDE3AQD2zTpElZaW1nr1Lj8/Xx999JFycnKUk5OjOXPm6KyzzlJubq42bdqk66+/XocffrjGjBmT0IEDACAxLwEAWp51iHr//fd14oknxv99zTXXSJKmTp2qhx9+WJ988okeffRRFRUVKS8vT6eccop+97vf8YoeAKBZMC8BAFqadYgaNWqUjDENXv+///u/TRoQAAA2mJcAAC2t2b8T5Wrnd7vk99sPrzISceoX9brVSVLAU+1cW1XpXKpAA6fmbUy1iTn39JS7v3Lrr4o61Zlww38cNd407FwarnYbb3l1VeMrNcAXdj+WiiLu+6m9P9OpLhjyOPdMbcJpbVJTAk51XjXheAi616a5P70oGA061aU63jXG/S4FmqRd+j1J6buj7Lqk9AVwYGv2s/MBAAAAwI8JIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALPiTPYCGfFfynbw++4xXXW3cGobS3eokpafEnGu9saBzbcxb6VRXXe1+t4dLSpxrYz63vp6I430qyev1ONeGY277N1rlVidJXq/78RBu2865NmLcXk9J9aU692yT4XOuTQu53a+eQMC5py8cda71N+Fx7vGWO9UFSlKc6sIVbv2AA5XfO7XFe0Zij7Z4TwCJxTtRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFghRAAAAAGCBEAUAAAAAFvzJHkBDinYUyeu1z3hRX7pTP18s4lQnSYFU9ywaiYada2PhSqe6qnL321paEnWujaSnOdV5vNXOPWMmxblWsSqnspDXvWfQVDjXBnwe59qsYNCprl2bts49c9q676e0kNtTVyhQ5twzGDbOtdGI2/6VJE+K221Nr3Ybb6nX/XYCB6LJ2Ze1eM/z265xrn26+CGnukjsUeeeAOrinSgAAAAAsECIAgAAAAALhCgAAAAAsECIAgAAAAALhCgAAAAAsECIAgAAAAALhCgAAAAAsECIAgAAAAALhCgAAAAAsECIAgAAAAALhCgAAAAAsECIAgAAAAALhCgAAAAAsOBP9gAaUlFeLI/HY10XC1Y69fMb913hDwSda9N8zqWKmJhTXUV5mXPPWIVxri2tdrtvjNd9J6UG3Mfrc2wb8bq/NuHzZzrXBtu2ca7NzDvEqS6rfZZzz0PSs51rM9PcHnOpKe7Hvr8Jx6EiIfe+gYBTXcAbcapLLW210wLQoDHB45M9hBYzOfsyp7rz265x7rlo93DnWuDHineiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALPiTPYCGRKNReTz2dZEq49bPt8epTpJSykPOtXt8zqWqrC5zqotUhp17ek3Mubaq0q0uGHM/TKuC7uP1h9z6mpDDgfv/HRJq61yb1SbLuTY9FHSq85lM556BjEOcazNS3R44AZ97T1+q+7Hkibk9L0mSL+R2W6urHJ8fIlGnOqCpiio+dS8OHp+4gaCO89uuSUrfRbuHJ6UvsD94JwoAAAAALBCiAAAAAMACIQoAAAAALFiFqLlz52ro0KHKzMxUhw4dNGHCBK1fv77WOpWVlZo5c6YOOeQQZWRk6KyzzlJhYWFCBw0AQA3mJgBAS7MKUatWrdLMmTP17rvv6tVXX1U4HNYpp5yisrJ/f4H56quv1ksvvaQlS5Zo1apV+vbbbzVx4sSEDxwAAIm5CQDQ8qxOP7Zs2bJa/164cKE6dOigtWvX6vjjj1dxcbH+/Oc/a/HixTrppJMkSQsWLNARRxyhd999V0cffXTiRg4AgJibAAAtr0nfiSouLpYk5eTkSJLWrl2rcDis0aNHx9fp06ePunbtqtWrV9e7jaqqKpWUlNS6AADgirkJANDcnENULBbTrFmzdOyxx6p///6SpIKCAgWDQbVp06bWuh07dlRBQUG925k7d66ys7Pjly5durgOCQBwkGNuAgC0BOcQNXPmTH366ad66qmnmjSAG2+8UcXFxfHLli1bmrQ9AMDBi7kJANASrL4TVePyyy/Xyy+/rDfeeEOdO3eOL8/NzVV1dbWKiopqveJXWFio3NzcercVCoUUCoVchgEAQBxzEwCgpVi9E2WM0eWXX66lS5fq9ddfV48ePWpdP2TIEAUCAS1fvjy+bP369fr66681YsSIxIwYAIAfYG4CALQ0q3eiZs6cqcWLF+uFF15QZmZm/LPk2dnZSk1NVXZ2tmbMmKFrrrlGOTk5ysrK0hVXXKERI0Zw9iMAQLNgbgIAtDSrEPXwww9LkkaNGlVr+YIFCzRt2jRJ0h/+8Ad5vV6dddZZqqqq0pgxY/TQQw8lZLAAAOyNuQkA0NKsQpQxptF1UlJSNG/ePM2bN895UAAA7C/mJgBAS3M6sURLiMgjjzzWdV6fW780X8StUJLC7j+3VVJd6t62vPE/HOrTlJvqMVHnWr/X/v6UJOOpcu5Z5T5cxaJtneqCGe2ce2bkZDnXpoUcD35Jxud2LO1xu0slSW087geiJ+j21OVtwrHk86c51wai7k+1gUi5U50nGnaqq3asA5pqcvZlyR4CWpnz265p8Z6Ldg9v8Z44MDXpx3YBAAAA4GBDiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC4QoAAAAALBAiAIAAAAAC/5kD6AhMROU5LGu8/gcc2HUrUySYrGIc211zDjXRqvd6vxp9vu1hql2r41G3W6r17jfOd5YE8YbDDvVxfxB554Rj/t4K8srnGt3lZY51YVC7vdNdqX746Y85HbfBFLdH2/yuj9dpoXcX69yPZr8ijnVxQJudUCNMcHjkz0EwNn5bde0eM9Fu4e3eE80He9EAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFQhQAAAAAWCBEAQAAAIAFf7IH0JBA1MjjcSiMuhRJMVPlVCdJPkWca6PVMfda43Zbq6IB555er3GuNRG32li1e9ZPDTbhvjFufX1V1c49TcUu59pdu9wfzmm+FKe6YGS3c8827ds615Y5HvtBhZx7+vzutd5M51J5K9z6eoNuTb1BXluDNCZ4fLKHABw0zm+7xrn26eKHnGsjsUeda8E7UQAAAABghRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABggRAFAAAAABYIUQAAAABgwZ/sAezNGFPrvw4baFJfp1o1obYpfZ13UbLG617rKtak8cbcekYjzj0jkbBzbbi62rm2uqrSqa6yssK5Z3lZmXNtadBtHwdi7vvIeNxfcwo4V0rBynKnuqpKt/t0T+n390syHq+t2b/npmiSR9Iywsb9sQKgJbk/V5eUuM0vLa2k5Pu/NVrbvOQxrWxEW7duVZcuXZI9DAA4qG3ZskWdO3dO9jBaDeYmAEiu1jYvtboQFYvF9O233yozM1Mej6fO9SUlJerSpYu2bNmirKysJIyw9WMfNY59tH/YT437se0jY4z27NmjvLw8eb184rvGvuamH9sx0FzYT41jH+0f9lPjfkz7qLXOS63u43xer3e/UmZWVtYBf1A0N/ZR49hH+4f91Lgf0z7Kzs5O9hBanf2Zm35Mx0BzYj81jn20f9hPjfux7KPWOC+1njgHAAAAAAcAQhQAAAAAWDjgQlQoFNLs2bMVCoWSPZRWi33UOPbR/mE/NY59BI6B/cN+ahz7aP+wnxrHPmp+re7EEgAAAADQmh1w70QBAAAAQDIRogAAAADAAiEKAAAAACwQogAAAADAwgEVoubNm6fu3bsrJSVFw4cP1z//+c9kD6lVufXWW+XxeGpd+vTpk+xhJdUbb7yh8ePHKy8vTx6PR88//3yt640xuuWWW9SpUyelpqZq9OjR2rBhQ3IGm0SN7adp06bVObZOPfXU5Aw2CebOnauhQ4cqMzNTHTp00IQJE7R+/fpa61RWVmrmzJk65JBDlJGRobPOOkuFhYVJGjFaEnNTw5iX6sfc1DjmpcYxNyXXAROinn76aV1zzTWaPXu2PvjgAw0cOFBjxozRd999l+yhtSr9+vXTtm3b4pe33nor2UNKqrKyMg0cOFDz5s2r9/q7775b999/v+bPn681a9YoPT1dY8aMUWVlZQuPNLka20+SdOqpp9Y6tp588skWHGFyrVq1SjNnztS7776rV199VeFwWKeccorKysri61x99dV66aWXtGTJEq1atUrffvutJk6cmMRRoyUwNzWOeaku5qbGMS81jrkpycwBYtiwYWbmzJnxf0ejUZOXl2fmzp2bxFG1LrNnzzYDBw5M9jBaLUlm6dKl8X/HYjGTm5tr7rnnnviyoqIiEwqFzJNPPpmEEbYOe+8nY4yZOnWqOeOMM5Iyntbou+++M5LMqlWrjDHfHzeBQMAsWbIkvs66deuMJLN69epkDRMtgLlp35iXGsfc1Djmpf3D3NSyDoh3oqqrq7V27VqNHj06vszr9Wr06NFavXp1EkfW+mzYsEF5eXnq2bOnzj//fH399dfJHlKrlZ+fr4KCglrHVXZ2toYPH85xVY+VK1eqQ4cO6t27ty699FLt3Lkz2UNKmuLiYklSTk6OJGnt2rUKh8O1jqU+ffqoa9euHEs/YsxN+4d5yQ5z0/5jXqqNuallHRAhaseOHYpGo+rYsWOt5R07dlRBQUGSRtX6DB8+XAsXLtSyZcv08MMPKz8/X8cdd5z27NmT7KG1SjXHDsdV40499VQ99thjWr58ue666y6tWrVKY8eOVTQaTfbQWlwsFtOsWbN07LHHqn///pK+P5aCwaDatGlTa12OpR835qbGMS/ZY27aP8xLtTE3tTx/sgeAxBk7dmz8/wcMGKDhw4erW7dueuaZZzRjxowkjgwHuilTpsT//8gjj9SAAQN02GGHaeXKlTr55JOTOLKWN3PmTH366ad8rwPYD8xLaC7MS7UxN7W8A+KdqHbt2snn89U5m0hhYaFyc3OTNKrWr02bNvrJT36ijRs3JnsorVLNscNxZa9nz55q167dQXdsXX755Xr55Ze1YsUKde7cOb48NzdX1dXVKioqqrU+x9KPG3OTPealxjE3uTlY5yWJuSlZDogQFQwGNWTIEC1fvjy+LBaLafny5RoxYkQSR9a6lZaWatOmTerUqVOyh9Iq9ejRQ7m5ubWOq5KSEq1Zs4bjqhFbt27Vzp07D5pjyxijyy+/XEuXLtXrr7+uHj161Lp+yJAhCgQCtY6l9evX6+uvv+ZY+hFjbrLHvNQ45iY3B9u8JDE3JdsB83G+a665RlOnTtVRRx2lYcOG6b777lNZWZmmT5+e7KG1Gtdee63Gjx+vbt266dtvv9Xs2bPl8/l07rnnJntoSVNaWlrrVan8/Hx99NFHysnJUdeuXTVr1izdfvvt6tWrl3r06KGbb75ZeXl5mjBhQvIGnQT72k85OTmaM2eOzjrrLOXm5mrTpk26/vrrdfjhh2vMmDFJHHXLmTlzphYvXqwXXnhBmZmZ8c+SZ2dnKzU1VdnZ2ZoxY4auueYa5eTkKCsrS1dccYVGjBiho48+OsmjR3Nibto35qX6MTc1jnmpccxNSZbs0wPaeOCBB0zXrl1NMBg0w4YNM++++26yh9SqTJ482XTq1MkEg0Fz6KGHmsmTJ5uNGzcme1hJtWLFCiOpzmXq1KnGmO9PJXvzzTebjh07mlAoZE4++WSzfv365A46Cfa1n8rLy80pp5xi2rdvbwKBgOnWrZu56KKLTEFBQbKH3WLq2zeSzIIFC+LrVFRUmMsuu8y0bdvWpKWlmTPPPNNs27YteYNGi2FuahjzUv2YmxrHvNQ45qbk8hhjTPNHNQAAAAD4cTggvhMFAAAAAK0FIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALBCiAAAAAMACIQoAAAAALPw/TdsFZMVbRbEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(dataset))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        # self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        # input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSat Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        '''\n",
    "        # Transformer Encoder\n",
    "\n",
    "        # PyTorch Encoder\n",
    "        # self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        # self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        # DeepSat Encoder\n",
    "        self.encoder = Transformer(self.d, self.num_layers, self.num_head, 32, self.d*4)\n",
    "\n",
    "\n",
    "        # torchvision Encoder\n",
    "        # self.encoder = Encoder(seq_length=self.N, num_heads=4, num_layers=4, hidden_dim=self.d, mlp_dim=self.d*4, dropout=0., attention_dropout=0.)\n",
    "\n",
    "\n",
    "        # Patches\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.N, self.K, self.d)) # (N, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Tekenization\n",
    "\n",
    "        Convert the images to a sequence of patches\n",
    "        '''\n",
    "        x_sits = x[:, :, :-1, :, :] # (B, T, C, H, W) -- > Exclude DOY Channel\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.reshape(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N)  we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "\n",
    "        (DOY -> One-Hot -> Projection)\n",
    "        '''\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W) in the last channel lies the DOY feature\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d) (DOY, one-hot encoded to represent the DOY feature and then encoded to d dimensions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "\n",
    "        add temporal embeddings (N*K) to the Time Series patches (T)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, 1, 1, 1) # (B, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.reshape(B*self.N, self.K, self.d) # (B*N, K, d)\n",
    "        x = x.reshape(B*self.N, T, self.d) # (B*N, T, d)\n",
    "        # Temporal Tokens (N*K)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=1) # (B*N, K+T, d)\n",
    "        # Temporal Transformer\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.reshape(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "        x = x.reshape(B*(self.K), self.N, self.d) # (B*K, N, d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        x = x + Ps # (B*K, N, d)\n",
    "        '''\n",
    "        # For Classification Only\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        '''\n",
    "        x = self.spatial_transformer(x) # (B*K, N, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, N, K, d)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Segmentation Head\n",
    "        '''\n",
    "        # classes = x[:,:,0,:] # (B, K, d)\n",
    "        # x = x[:,:,1:,:] # (B, K, N, d)\n",
    "        \n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Reassemble\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1) # (B, N, P*P, K)\n",
    "        x = x.reshape(B, self.N, self.P, self.P, self.K) # (B, N, P, P, K)\n",
    "        x = x.reshape(B, self.H, self.W, self.K) # (B, H, W, K)\n",
    "        # x = x.permute(0, 3, 1, 2) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  2339721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoder): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (spatial_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 8\n",
    "dataset = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_samples = dataset.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=8, num_layers=8, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(dataset):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'./weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": Accuracy(),\n",
    "    # \"precision\": Precision(),\n",
    "    # \"recall\": Recall(),\n",
    "    # \"confusion_matrix\": ConfusionMatrix(num_classes=20),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    # y_pred = torch.argmax(y_pred, dim=3)\n",
    "    # print(y.shape, y_pred.shape)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        y = F.one_hot(y, num_classes=20)\n",
    "        y_pred = F.one_hot(torch.argmax(model(x), dim=3), num_classes=20).astype(int)\n",
    "        # y_pred = y_pred.permute(0, 3, 1, 2)\n",
    "        print(y.shape, y_pred.shape)\n",
    "        return y_pred, y\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "val_evaluator = Engine(validation_step)\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(val_evaluator, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many batches to wait before logging training status\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f217d6d5410>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score function to return current value of any metric we defined above in val_metrics\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "# Checkpoint to store n_saved best models wrt score function\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer), # helps fetch the trainer's state\n",
    ")\n",
    "  \n",
    "# Save the model after every epoch of val_evaluator is completed\n",
    "val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")\n",
    "\n",
    "# Attach handler to plot trainer's loss every 100 iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.EPOCH_COMPLETED(every=100),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[ACurrent run is terminating due to exception: 'Tensor' object has no attribute 'astype'\n",
      "Engine run is terminating due to exception: 'Tensor' object has no attribute 'astype'\n",
      "Engine run is terminating due to exception: 'Tensor' object has no attribute 'astype'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pbar \u001b[39m=\u001b[39m ProgressBar()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pbar\u001b[39m.\u001b[39mattach(trainer)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun(train_loader, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n\u001b[1;32m    893\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator)\n\u001b[1;32m    936\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:965\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m epoch_time_taken\n\u001b[1;32m    964\u001b[0m handlers_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 965\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fire_event(Events\u001b[39m.\u001b[39;49mEPOCH_COMPLETED)\n\u001b[1;32m    966\u001b[0m epoch_time_taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m handlers_start_time\n\u001b[1;32m    967\u001b[0m \u001b[39m# update time wrt handlers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:425\u001b[0m, in \u001b[0;36mEngine._fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(event_kwargs)\n\u001b[1;32m    424\u001b[0m first, others \u001b[39m=\u001b[39m ((args[\u001b[39m0\u001b[39m],), args[\u001b[39m1\u001b[39m:]) \u001b[39mif\u001b[39;00m (args \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m) \u001b[39melse\u001b[39;00m ((), args)\n\u001b[0;32m--> 425\u001b[0m func(\u001b[39m*\u001b[39;49mfirst, \u001b[39m*\u001b[39;49m(event_args \u001b[39m+\u001b[39;49m others), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 35\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@trainer\u001b[39m\u001b[39m.\u001b[39mon(Events\u001b[39m.\u001b[39mEPOCH_COMPLETED)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_training_results\u001b[39m(trainer):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_evaluator\u001b[39m.\u001b[39;49mrun(train_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     metrics \u001b[39m=\u001b[39m train_evaluator\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Results - Epoch[\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch\u001b[39m}\u001b[39;00m\u001b[39m] Avg accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Avg loss: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n\u001b[1;32m    893\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator)\n\u001b[1;32m    936\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:959\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 959\u001b[0m epoch_time_taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    961\u001b[0m \u001b[39m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:1087\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1087\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m   1089\u001b[0m \u001b[39mreturn\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:1068\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_STARTED)\n\u001b[1;32m   1066\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m-> 1068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_function(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m   1069\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m   1070\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x, y \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(y, num_classes\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y_pred \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mone_hot(torch\u001b[39m.\u001b[39;49margmax(model(x), dim\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m), num_classes\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# y_pred = y_pred.permute(0, 3, 1, 2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape, y_pred\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer)\n",
    "\n",
    "trainer.run(train_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(dataset))\n",
    "output = torch.argmax(model(img.to(device)), dim=3)\n",
    "\n",
    "fix, axes = plt.subplots(1,3, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "axes[2].imshow(output[0].cpu().numpy(), cmap='inferno')\n",
    "\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'Label: {label[0].unique().tolist()}')\n",
    "axes[2].set_title(f'Prediction: {output[0].cpu().unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
