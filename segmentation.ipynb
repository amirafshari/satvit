{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTIS24 = './data/PASTIS24/'\n",
    "PASTIS9 = './data/PASTIS9/'\n",
    "\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24) [[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 0. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0.]\n",
      " [3. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0.]]\n",
      "DOY:  (43,) [ 17  22  47  52  57  72  87 102 112 117 132 147 152 157 172 177 182 187\n",
      " 192 197 212 222 227 232 237 242 247 257 262 267 267 272 277 282 282 292\n",
      " 292 297 302 312 317 322 327]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape, data['labels'])\n",
    "print('DOY: ', data['doy'].shape, data['doy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)[:500]\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0,  1,  2,  3,  6, 10, 17, 19])')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(dataset))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSat Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class PreNormLocal(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        # print('before fn: ', x.shape)\n",
    "        x = self.fn(x, **kwargs)\n",
    "        # print('after fn: ', x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1x1Block(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(hidden_dim, dim, kernel_size=1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.reattn_weights = nn.Parameter(torch.randn(heads, heads))\n",
    "\n",
    "        self.reattn_norm = nn.Sequential(\n",
    "            Rearrange('b h i j -> b i j h'),\n",
    "            nn.LayerNorm(heads),\n",
    "            Rearrange('b i j h -> b h i j')\n",
    "        )\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # re-attention\n",
    "\n",
    "        attn = einsum('b h i j, h g -> b g i j', attn, self.reattn_weights)\n",
    "        attn = self.reattn_norm(attn)\n",
    "\n",
    "        # aggregate and out\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LeFF(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=192, scale=4, depth_kernel=3):\n",
    "        super().__init__()\n",
    "\n",
    "        scale_dim = dim * scale\n",
    "        self.up_proj = nn.Sequential(nn.Linear(dim, scale_dim),\n",
    "                                     Rearrange('b n c -> b c n'),\n",
    "                                     nn.BatchNorm1d(scale_dim),\n",
    "                                     nn.GELU(),\n",
    "                                     Rearrange('b c (h w) -> b c h w', h=14, w=14)\n",
    "                                     )\n",
    "\n",
    "        self.depth_conv = nn.Sequential(\n",
    "            nn.Conv2d(scale_dim, scale_dim, kernel_size=depth_kernel, padding=1, groups=scale_dim, bias=False),\n",
    "            nn.BatchNorm2d(scale_dim),\n",
    "            nn.GELU(),\n",
    "            Rearrange('b c h w -> b (h w) c', h=14, w=14)\n",
    "            )\n",
    "\n",
    "        self.down_proj = nn.Sequential(nn.Linear(scale_dim, dim),\n",
    "                                       Rearrange('b n c -> b c n'),\n",
    "                                       nn.BatchNorm1d(dim),\n",
    "                                       nn.GELU(),\n",
    "                                       Rearrange('b c n -> b n c')\n",
    "                                       )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up_proj(x)\n",
    "        x = self.depth_conv(x)\n",
    "        x = self.down_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LCAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        q = q[:, :, -1, :].unsqueeze(2)  # Only Lth element use as query\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        '''\n",
    "        # Transformer Encoder\n",
    "        # self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        # self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.encoder = Transformer(self.d, self.num_layers, self.num_head, 32, self.d*4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Patches\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.N, self.K, self.d)) # (N, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Tekenization\n",
    "\n",
    "        Convert the images to a sequence of patches\n",
    "        '''\n",
    "        x_sits = x[:, :, :-1, :, :] # (B, T, C, H, W) -- > Exclude DOY Channel\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.reshape(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N)  we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "\n",
    "        (DOY -> One-Hot -> Projection)\n",
    "        '''\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W) in the last channel lies the DOY feature\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d) (DOY, one-hot encoded to represent the DOY feature and then encoded to d dimensions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "\n",
    "        add temporal embeddings (N*K) to the Time Series patches (T)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, 1, 1, 1) # (B, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.reshape(B*self.N, self.K, self.d) # (B*N, K, d)\n",
    "        x = x.reshape(B*self.N, T, self.d) # (B*N, T, d)\n",
    "        # Temporal Tokens (N*K)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=1) # (B*N, K+T, d)\n",
    "        # Temporal Transformer\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.reshape(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "        x = x.reshape(B*(self.K), self.N, self.d) # (B*K, N, d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        x = x + Ps # (B*K, N, d)\n",
    "        '''\n",
    "        # For Classification Only\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        '''\n",
    "        x = self.spatial_transformer(x) # (B*K, N, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, N, K, d)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Segmentation Head\n",
    "        '''\n",
    "        # classes = x[:,:,0,:] # (B, K, d)\n",
    "        # x = x[:,:,1:,:] # (B, K, N, d)\n",
    "        \n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Reassemble\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1) # (B, N, P*P, K)\n",
    "        x = x.reshape(B, self.N, self.P, self.P, self.K) # (B, N, P, P, K)\n",
    "        x = x.reshape(B, self.H, self.W, self.K) # (B, H, W, K)\n",
    "        # x = x.permute(0, 3, 1, 2) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  2339721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoder): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (spatial_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 2\n",
    "dataset = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "num_samples = dataset.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=8, num_layers=8, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:25<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  tensor(129.8928, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  tensor(112.4386, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 Loss:  tensor(109.8572, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:25<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 Loss:  tensor(107.8857, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:25<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 Loss:  tensor(105.8001, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 Loss:  tensor(105.8053, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6 Loss:  tensor(104.7655, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7 Loss:  tensor(103.0245, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8 Loss:  tensor(101.7697, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9 Loss:  tensor(100.5096, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10 Loss:  tensor(100.3528, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11 Loss:  tensor(98.3200, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12 Loss:  tensor(97.5872, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13 Loss:  tensor(96.1902, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14 Loss:  tensor(94.6718, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15 Loss:  tensor(94.7932, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:24<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16 Loss:  tensor(93.4093, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:25<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17 Loss:  tensor(90.9597, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 91/250 [00:09<00:15,  9.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   img, label \u001b[39m=\u001b[39m batch\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   img, label \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mto(device), label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   output \u001b[39m=\u001b[39m model(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(dataset):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'./weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
