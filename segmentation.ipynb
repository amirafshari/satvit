{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASTIS24 = './data/PASTIS24/'\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",
      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24)\n",
      "DOY:  (43,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape)\n",
    "print('DOY: ', data['doy'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7675"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = random_split(data, [int(.8*len(data)), int(.2*len(data))])\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0,  1,  2,  3,  4,  5,  9, 19])')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(train_loader))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        # self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        # input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSat Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4, dropoutratio=0.5\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        '''\n",
    "        # Transformer Encoder\n",
    "\n",
    "        # PyTorch Encoder\n",
    "        # self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        # self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        # DeepSat Encoder\n",
    "        self.encoder = Transformer(self.d, self.num_layers, self.num_head, 32, self.d*4, dropoutratio)\n",
    "\n",
    "        # torchvision Encoder\n",
    "        # self.encoder = Encoder(seq_length=self.N, num_heads=4, num_layers=4, hidden_dim=self.d, mlp_dim=self.d*4, dropout=0., attention_dropout=0.)\n",
    "\n",
    "        # Patches\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.N, self.K, self.d)) # (N, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropoutratio)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Tekenization\n",
    "\n",
    "        Convert the images to a sequence of patches\n",
    "        '''\n",
    "        x_sits = x[:, :, :-1, :, :] # (B, T, C, H, W) -- > Exclude DOY Channel\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        # x_sits = self.dropout(x_sits)\n",
    "        x_sits = x_sits.reshape(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N)  we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "\n",
    "        (DOY -> One-Hot -> Projection)\n",
    "        '''\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W) in the last channel lies the DOY feature\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d) (DOY, one-hot encoded to represent the DOY feature and then encoded to d dimensions)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "\n",
    "        add temporal embeddings (N*K) to the Time Series patches (T)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        # x = self.dropout(x)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, 1, 1, 1) # (B, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.reshape(B*self.N, self.K, self.d) # (B*N, K, d)\n",
    "        x = x.reshape(B*self.N, T, self.d) # (B*N, T, d)\n",
    "        # Temporal Tokens (N*K)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=1) # (B*N, K+T, d)\n",
    "        # Temporal Transformer\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.reshape(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "        x = x.reshape(B*(self.K), self.N, self.d) # (B*K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        x = x + Ps # (B*K, N, d)\n",
    "        x = self.dropout(x)\n",
    "        '''\n",
    "        # For Classification Only\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        '''\n",
    "        x = self.spatial_transformer(x) # (B*K, N, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, N, K, d)\n",
    "\n",
    "        '''\n",
    "        Segmentation Head\n",
    "        '''\n",
    "        # classes = x[:,:,0,:] # (B, K, d)\n",
    "        # x = x[:,:,1:,:] # (B, K, N, d)\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        '''\n",
    "        Reassemble\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1) # (B, N, P*P, K)\n",
    "        x = x.reshape(B, self.N, self.P, self.P, self.K) # (B, N, P, P, K)\n",
    "        x = x.reshape(B, self.H, self.W, self.K) # (B, H, W, K)\n",
    "        # x = x.permute(0, 3, 1, 2) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  1023881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoder): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (spatial_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 8\n",
    "train_set, val_set = random_split(data, [int(.8*len(data)), int(.2*len(data))])\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=True)\n",
    "num_samples = train_loader.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=4, num_layers=4, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 768/768 [01:41<00:00,  7.53it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /workspace/weights does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   torch\u001b[39m.\u001b[39;49msave({\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m: epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m: model\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39moptimizer_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m: optimizer\u001b[39m.\u001b[39;49mstate_dict(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m: loss,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             }, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/workspace/weights/epoch_\u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X33sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m'\u001b[39m, epoch, \u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m'\u001b[39m, (epoch_loss\u001b[39m/\u001b[39mnum_samples_train)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /workspace/weights does not exist."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(train_loader):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'/workspace/weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "\n",
    "  \n",
    "    \n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples_train)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training milad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, WeightsScalarHandler, WeightsHistHandler, GradsScalarHandler\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.handlers import LRScheduler\n",
    "from ignite.metrics import RunningAverage\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Metric\n",
    "from ignite.metrics.metric import sync_all_reduce, reinit__is_reduced\n",
    "\n",
    "class CustomAccuracy(Metric):\n",
    "    def __init__(self, output_transform=lambda x: x):\n",
    "        super(CustomAccuracy, self).__init__(output_transform=output_transform)\n",
    "\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self._num_correct = 0\n",
    "        self._num_examples = 0\n",
    "\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        predicted_classes = torch.argmax(y_pred, dim=-1)\n",
    "        correct = (predicted_classes == y).float().sum().item()\n",
    "        self._num_correct += correct\n",
    "        self._num_examples += y.numel()\n",
    "\n",
    "    @sync_all_reduce(\"_num_correct\", \"_num_examples\")\n",
    "    def compute(self):\n",
    "        return self._num_correct / self._num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  # Your model\n",
    "train_loader = train_loader  # Your training data loader\n",
    "val_loader = train_loader  # Your validation data loader\n",
    "test_loader = train_loader  # Your test data loader\n",
    "optimizer = optimizer  # Your optimizer\n",
    "criterion = criterion  # Your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger('ignite.engine.engine.Engine').setLevel(logging.WARNING)  # <-- Set logging level to WARNING\n",
    "\n",
    "def setup_directories(log_dir, model_save_dir):\n",
    "    \"\"\"Setup directories if they don't exist.\"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    if not os.path.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, max_epochs=100, log_dir='./tb_logs', model_save_dir='./checkpoints'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Create trainer and evaluator\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    evaluator = create_supervised_evaluator(model, metrics={\"accuracy\": CustomAccuracy(), \"loss\": Loss(criterion)}, device=device)\n",
    "\n",
    "    # Tensorboard Logger\n",
    "    tb_logger = TensorboardLogger(log_dir=log_dir)\n",
    "\n",
    "    # Attach various logging handlers to Tensorboard for training\n",
    "    tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", output_transform=lambda loss: {'loss': loss}),\n",
    "                     event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=WeightsScalarHandler(model), event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=GradsScalarHandler(model), event_name=Events.ITERATION_COMPLETED)\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    scheduler = LRScheduler(lr_scheduler)\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED, scheduler)\n",
    "\n",
    "    # Model Checkpointing\n",
    "    checkpoint_handler = ModelCheckpoint(model_save_dir, 'model', n_saved=3, require_empty=False)\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': model})\n",
    "\n",
    "    # Early Stopping\n",
    "    def score_function(engine):\n",
    "        val_loss = engine.state.metrics['loss']\n",
    "        return -val_loss\n",
    "\n",
    "    handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n",
    "    evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "    # Save the first model after the first epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(once=1))\n",
    "    def save_first_model(engine):\n",
    "        epoch = engine.state.epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'first_model_epoch_{epoch}.pth'))\n",
    "\n",
    "    # Save the last model after each epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def save_last_model(engine):\n",
    "        epoch = engine.state.epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'last_model_epoch_{epoch}.pth'))\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    best_accuracy = 0.0\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def save_best_model(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        accuracy = evaluator.state.metrics['accuracy']\n",
    "        nonlocal best_accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            epoch = engine.state.epoch\n",
    "            torch.save(model.state_dict(), os.path.join(model_save_dir, f'best_model_epoch_{epoch}.pth'))\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    # Log training loss after each iteration\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_iteration(engine):\n",
    "        iteration = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "        print(f\"Epoch[{engine.state.epoch}] Iteration[{iteration}/{len(train_loader)}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "    # Log training and validation results at the end of each epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_epoch_results(engine):\n",
    "        # Training metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        evaluator.run(train_loader)\n",
    "        train_metrics = evaluator.state.metrics\n",
    "        print(f\"Training Results - Epoch: {engine.state.epoch} Avg accuracy: {train_metrics['accuracy']:.2f} Avg loss: {train_metrics['loss']:.2f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        evaluator.run(val_loader)\n",
    "        val_metrics = evaluator.state.metrics\n",
    "        print(f\"Validation Results - Epoch: {engine.state.epoch} Avg accuracy: {val_metrics['accuracy']:.2f} Avg loss: {val_metrics['loss']:.2f}\")\n",
    "\n",
    "        model.train()  # Set model back to training mode\n",
    "\n",
    "    # Run the training loop\n",
    "    trainer.run(train_loader, max_epochs=max_epochs)\n",
    "\n",
    "    # Close the logger\n",
    "    tb_logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Files ['best_model_2_accuracy=0.5957.pt', 'best_model_2_accuracy=0.5845.pt'] with extension '.pt' are already present in the directory checkpoint. If you want to use this directory anyway, pass `require_empty=False`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 33\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m engine\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mmetrics[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Checkpoint to store n_saved best models wrt score function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcheckpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     n_saved\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     filename_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     score_function\u001b[39m=\u001b[39;49mscore_function,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     score_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     global_step_transform\u001b[39m=\u001b[39;49mglobal_step_from_engine(trainer), \u001b[39m# helps fetch the trainer's state\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# Save the model after every epoch of val_evaluator is completed\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X44sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m evaluator\u001b[39m.\u001b[39madd_event_handler(Events\u001b[39m.\u001b[39mCOMPLETED, model_checkpoint, {\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model})\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:998\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, dirname, filename_prefix, save_interval, score_function, score_name, n_saved, atomic, require_empty, create_dir, save_as_state_dict, global_step_transform, archived, filename_pattern, include_self, greater_or_equal, save_on_rank, **kwargs)\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m         \u001b[39m# No choice\u001b[39;00m\n\u001b[1;32m    996\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 998\u001b[0m disk_saver \u001b[39m=\u001b[39m DiskSaver(\n\u001b[1;32m    999\u001b[0m     dirname,\n\u001b[1;32m   1000\u001b[0m     atomic\u001b[39m=\u001b[39;49matomic,\n\u001b[1;32m   1001\u001b[0m     create_dir\u001b[39m=\u001b[39;49mcreate_dir,\n\u001b[1;32m   1002\u001b[0m     require_empty\u001b[39m=\u001b[39;49mrequire_empty,\n\u001b[1;32m   1003\u001b[0m     save_on_rank\u001b[39m=\u001b[39;49msave_on_rank,\n\u001b[1;32m   1004\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1005\u001b[0m )\n\u001b[1;32m   1007\u001b[0m \u001b[39msuper\u001b[39m(ModelCheckpoint, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m   1008\u001b[0m     to_save\u001b[39m=\u001b[39m{},\n\u001b[1;32m   1009\u001b[0m     save_handler\u001b[39m=\u001b[39mdisk_saver,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     save_on_rank\u001b[39m=\u001b[39msave_on_rank,\n\u001b[1;32m   1020\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:810\u001b[0m, in \u001b[0;36mDiskSaver.__init__\u001b[0;34m(self, dirname, atomic, create_dir, require_empty, save_on_rank, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_on_rank \u001b[39m=\u001b[39m save_on_rank\n\u001b[1;32m    809\u001b[0m \u001b[39mif\u001b[39;00m idist\u001b[39m.\u001b[39mget_rank() \u001b[39m==\u001b[39m save_on_rank:\n\u001b[0;32m--> 810\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_and_setup(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdirname, create_dir, require_empty)\n\u001b[1;32m    811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/handlers/checkpoint.py:825\u001b[0m, in \u001b[0;36mDiskSaver._check_and_setup\u001b[0;34m(dirname, create_dir, require_empty)\u001b[0m\n\u001b[1;32m    823\u001b[0m matched \u001b[39m=\u001b[39m [fname \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dirname) \u001b[39mif\u001b[39;00m fname\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    824\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(matched) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    826\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFiles \u001b[39m\u001b[39m{\u001b[39;00mmatched\u001b[39m}\u001b[39;00m\u001b[39m with extension \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are already present \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    827\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the directory \u001b[39m\u001b[39m{\u001b[39;00mdirname\u001b[39m}\u001b[39;00m\u001b[39m. If you want to use this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    828\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdirectory anyway, pass `require_empty=False`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    829\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Files ['best_model_2_accuracy=0.5957.pt', 'best_model_2_accuracy=0.5845.pt'] with extension '.pt' are already present in the directory checkpoint. If you want to use this directory anyway, pass `require_empty=False`."
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar\n",
    "from utils.accuracy import CustomAccuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": CustomAccuracy(),\n",
    "    # \"precision\": Precision(),\n",
    "    # \"recall\": Recall(),\n",
    "    # \"confusion_matrix\": ConfusionMatrix(num_classes=20),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    # y_pred = torch.argmax(y_pred, dim=3)\n",
    "    # print(y.shape, y_pred.shape)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        # y = F.one_hot(y, num_classes=20)\n",
    "        y_pred = model(x)\n",
    "        # y_pred = F.one_hot(torch.argmax(model(x), dim=3), num_classes=20).astype(int)\n",
    "        # y_pred = y_pred.permute(0, 3, 1, 2)\n",
    "        # print(y.shape, y_pred.shape)\n",
    "        return y_pred, y\n",
    "\n",
    "\n",
    "evaluator = Engine(validation_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(evaluator, name)\n",
    "\n",
    "\n",
    "\n",
    "# How many batches to wait before logging training status\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Score function to return current value of any metric we defined above in val_metrics\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "# Checkpoint to store n_saved best models wrt score function\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer), # helps fetch the trainer's state\n",
    ")\n",
    "  \n",
    "# Save the model after every epoch of val_evaluator is completed\n",
    "evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")\n",
    "\n",
    "# Attach handler to plot trainer's loss every 100 iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.EPOCH_COMPLETED(every=100),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "for tag, evaluator in [(\"training\", evaluator), (\"validation\", evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer)\n",
    "\n",
    "trainer.run(train_loader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(val_loader))\n",
    "model.eval()\n",
    "output = torch.argmax(model(img.to(device)), dim=3)\n",
    "\n",
    "fix, axes = plt.subplots(1,3, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "axes[2].imshow(output[0].cpu().numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'Label: {label[0].unique().tolist()}')\n",
    "axes[2].set_title(f'Prediction: {output[0].cpu().unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "checkpoint = torch.load('weights/epoch_80.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
