{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CutOrPad, get_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [

    "PASTIS24 = './data/PASTIS24/'\n",
    "PATH = PASTIS24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PATH)\n",
    "file = random.choice(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['img', 'labels', 'doy'])\n",

      "Image:  (43, 10, 24, 24)\n",
      "Labels:  (24, 24) [[2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n",
      "DOY:  (43,) [ 17  22  47  52  57  72  87 102 112 117 132 147 152 157 172 177 182 187\n",
      " 192 197 212 222 227 232 237 242 247 257 262 267 267 272 277 282 282 292\n",
      " 292 297 302 312 317 322 327]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle(PATH + file)\n",
    "\n",
    "print(data.keys())\n",
    "print('Image: ', data['img'].shape)\n",
    "print('Labels: ', data['labels'].shape)\n",
    "print('DOY: ', data['doy'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PASTIS(Dataset):\n",
    "    def __init__(self, pastis_path):\n",
    "        self.pastis_path = pastis_path\n",
    "\n",
    "        self.file_names = os.listdir(self.pastis_path)\n",
    "\n",
    "        random.shuffle(self.file_names)\n",
    "\n",
    "        self.to_cutorpad = CutOrPad()\n",
    "        # self.to_tiledates = TileDates(24, 24)\n",
    "        # self.to_unkmask = UnkMask(unk_class=19, ground_truth_target='labels'))\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "\n",
    "    def add_date_channel(self, img, doy):\n",
    "        img = torch.cat((img, doy), dim=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def normalize(self, img):\n",
    "        C = img.shape[1]\n",
    "        mean = img.mean(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "        std = img.std(dim=(0, 2, 3)).to(torch.float32).reshape(1, C, 1, 1)\n",
    "\n",
    "        img = (img - mean) / std\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = pd.read_pickle(os.path.join(self.pastis_path, self.file_names[idx]))\n",
    "\n",
    "        data['img'] = data['img'].astype('float32')\n",
    "        data['img'] = torch.tensor(data['img'])\n",
    "        data['img'] = self.normalize(data['img'])\n",
    "        T, C, H, W = data['img'].shape\n",
    "\n",
    "        data['labels'] = data['labels'].astype('long')\n",
    "        data['labels'] = torch.tensor(data['labels'])\n",
    "        # data['labels'] = F.one_hot(data['labels'].long(), num_classes=20)\n",
    "\n",
    "        data['doy'] = data['doy'].astype('float32')\n",
    "        data['doy'] = torch.tensor(data['doy'])\n",
    "        data['doy'] = data['doy'].unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "        data['doy'] = data['doy'].repeat(1, 1, H, W)\n",
    "\n",
    "        data['img'] = self.add_date_channel(data['img'], data['doy']) # add DOY to the last channel\n",
    "        del data['doy'] # Delete DOY\n",
    "\n",
    "        data = self.to_cutorpad(data) # Pad to Max Sequence Length\n",
    "        del data['seq_lengths'] # Delete Sequence Length\n",
    "\n",
    "\n",
    "        return data['img'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PASTIS(PATH)\n",
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = random_split(data, [400, 100])\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},

   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tensor([ 0,  1,  2,  3,  4, 10, 14, 17, 19])')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAGiCAYAAAA1J1M9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/LklEQVR4nO3deXhU5d3/8c8kmZnsG5ANCIugkdWKBFEUEMryKI8Uq+BS0YfqrwW0SC2KVRCrUvV61KsVpa2t1gVrsVWrbWkVWVxAFEVcEAGDrFkIZF8mydy/P3gyJZCY5M4kkwPv13XNpcycz3zvOXNm7nznzDnjMsYYAQAAAICDhYV6AAAAAADQVjQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NkATnn76ablcLu3evTvUQwGATufBBx9UVlaW/H6/JGnt2rVyuVyBy4cffhjiEQLoKImJiYHX/ty5cwPXL1++XJmZmaquru6QcdDYAADQTt577z3dfffdKioqCvVQgqqkpEQPPPCAbrvtNoWFNfxT4o477tCzzz6rvn37Nns/fr9fDz74oPr06aPIyEgNGTJEL7zwQtDH++9//1uzZs3SoEGDFB4ert69ewe9hiR98MEHmjt3rgYOHKiYmBhlZmbqiiuu0FdffRX0Wvfff7/OPfdcdevWTZGRkerfv7/mzZungoKCoNc61n333SeXy6VBgwa1+X7++7//W6mpqXK5XLr77rubXHb//v264oorlJiYqPj4eF166aX6+uuvrWsfPHhQt99+u8aOHau4uDi5XC6tXbu22VxRUZFSUlLkcrn00ksvWdfftGmTZs+erWHDhsntdsvlcjW6XP0HrE1dnn/+eav6L774oq655hr1799fLpdLY8aMaXLZzZs3a9KkSYqPj1dcXJwmTJigLVu2nLDcb3/7Wz377LMnXH/dddfJ5/PpN7/5jdVYWyuiQ6oADvSDH/xAM2bMkNfrDfVQADjUe++9pyVLlui6665TYmJiqIcTNH/4wx9UW1urK6+88oTbvvvd737rH0rH+vnPf65f/vKXuuGGGzR8+HC9+uqruuqqq+RyuTRjxoygjXfFihV68cUXdfbZZysjIyNo93u8Bx54QO+++64uv/xyDRkyRLm5uXrsscd09tlna+PGjW1uBo61efNmnXXWWZoxY4bi4uK0bds2/e53v9Pf//53bdmyRTExMUGrVW/fvn26//77g3Lfd955p9LS0vSd73xH//rXv5pcrqysTGPHjlVxcbHuuOMOud1uPfLIIxo9erS2bNmiLl26tLr29u3b9cADD6h///4aPHiwNmzY0KLcokWLVFFR0ep6x/vHP/6hJ598UkOGDFHfvn2bbHwvvPDCRpuFRx55RJ988onGjRtnVf+JJ57Q5s2bNXz4cBUWFja53EcffaRRo0apZ8+eWrx4sfx+vx5//HGNHj1amzZt0hlnnBFY9oorrpB09G+nY0VGRmrmzJl6+OGHddNNNzXZxAWNAQAA7eKhhx4ykkxOTk6oh9JmZWVlgf8fMmSIueaaaxrcvmbNGiPJrFmzpkX3t2/fPuN2u82cOXMC1/n9fnPBBReYHj16mNra2qCM2xhj9u/fb3w+nzHGmIsvvtj06tUraPd9rHfffddUV1c3uO6rr74yXq/XXH311e1S81gvvfSSkWReeOGFdrn/6dOnm4suusiMHj3aDBw4sE33Vf+aKCgoMJLM4sWLG13ugQceMJLMpk2bAtdt27bNhIeHm4ULF1rVLikpMYWFhcYYY1auXNmi7fbTTz81ERER5p577jGSzMqVK61qG2NMbm6uqaioMMYYM2fOHNOaP8crKipMXFyc+e53v2tdf8+ePaaurs4YY8zAgQPN6NGjG13uv/7rv0xSUpI5dOhQ4LoDBw6Y2NhYM23atEYzkhq8po0x5sMPPzSSzOrVq63H3FJ8FQ1owvHH2PTu3VuXXHKJ1q5dq3POOUdRUVEaPHhwYPf1X//6Vw0ePFiRkZEaNmyYPv744xPuc+XKlRowYIAiIyM1aNAgvfzyy7ruuuva7WsRAELn7rvv1s9+9jNJUp8+fQJfHzn2uL3nnntOw4YNU1RUlJKTkzVjxgzt3bu3wf2MGTNGgwYN0hdffKGxY8cqOjpa3bt314MPPnhCzV//+tcaOHCgoqOjlZSUpHPOOUcrVqxosMzHH3+syZMnKz4+XrGxsRo3bpw2btzYYJn6979169Zp9uzZSklJUY8ePSRJOTk52rp1q8aPH9+m9fPqq6+qpqZGs2fPDlzncrn04x//WPv27Wvxp+gtkZGRIbfbHbT7a8p5550nj8fT4Lr+/ftr4MCB2rZtW7vXr59L2uOrj+vXr9dLL72kRx99NCj319J576WXXtLw4cM1fPjwwHVZWVkaN26c/vznP1vVjouLU3JycqsyP/nJT/S9731PF1xwgVXNY6WmpioqKsoq+9prr6m0tFRXX321df2ePXue8BXSxrz99tsaP358g71i6enpGj16tF5//XWVlZW1qN6wYcOUnJysV1991XrMLUVjA7TCzp07ddVVV2nKlClaunSpjhw5oilTpuj555/XLbfcomuuuUZLlizRrl27dMUVVwQOqpWkv//975o+fbrcbreWLl2qadOmadasWdq8eXMIHxGA9jJt2rTAV7UeeeQRPfvss3r22WfVrVs3SUePMbj22mvVv39/Pfzww5o3b55Wr16tCy+88IQ/TI8cOaJJkyZp6NCh+t///V9lZWXptttu0z//+c/AMr/73e908803a8CAAXr00Ue1ZMkSnXXWWXr//fcDy3z++ee64IIL9Mknn2jBggW66667lJOTozFjxjRYrt7s2bP1xRdfaNGiRbr99tslHf16nSSdffbZbVo/H3/8sWJiYnTmmWc2uD47Oztw+8nAGKO8vDx17dq1Xe770KFDys3N1dtvv62bb75Z4eHhLf4qYEvV1dXppptu0g9/+EMNHjw4qPf9bfx+v7Zu3apzzjnnhNuys7O1a9culZaWtvs4Vq5cqffee6/RDxM62vPPP6+oqChNmzat3WtVV1c32oBFR0fL5/Pps88+a/F9nX322Xr33XeDObxGcYwN0Arbt2/Xe++9p5EjR0qSBgwYoIkTJ+qGG27Ql19+qczMTElSUlKS/t//+39av359YIJZuHChunfvrnfffVexsbGSpHHjxmnMmDHq1atXSB4PgPYzZMgQnX322XrhhRc0derUBp9Qf/PNN1q8eLHuvfde3XHHHYHrp02bpu985zt6/PHHG1x/4MABPfPMM4Hvr8+aNUu9evXS73//e02ePFnS0Q9PBg4cqJUrVzY5pjvvvFM1NTV65513Agf3X3vttTrjjDO0YMECrVu3rsHyycnJWr16tcLDwwPXffnll5KO7oVqi4MHDwYOHD9Wenp64DGfDJ5//nnt379f99xzT9DvOy8vL7C+JKlHjx5asWKFsrKyglpn+fLl+uabb/Tmm28G9X6bc/jwYVVXVzd4jPWO3U6OPdYj2CorK3XrrbfqlltuUe/evUN6ptTDhw9r1apVmjp1quLi4tq93hlnnKGNGzeqrq4u8B7g8/kCH4Ls37+/xffVt2/fRo8XCjb22ACtMGDAgEBTI0kjRoyQJF100UWBpubY6+vP2nLgwAF9+umnuvbaawNNjSSNHj26Qz/9AtA5/PWvf5Xf79cVV1yhQ4cOBS5paWnq37+/1qxZ02D52NhYXXPNNYF/ezweZWdnNzgzVGJiovbt26cPPvig0Zp1dXX697//ralTpzY4Y1l6erquuuoqvfPOOyopKWmQueGGGxo0NZJUWFioiIiIBu9lNiorKxs9OUtkZGTgdqf78ssvNWfOHI0cOVIzZ84M+v0nJyfrjTfe0GuvvaZ77rlHXbt2bfHXg1qqsLBQixYt0l133RXY29hR6reBUG4nv/zlL1VTU9Pgg4ZQeemll+Tz+dr0NbTWmD17tr766ivNmjVLX3zxhT777DNde+21OnjwoKTWrfukpCRVVlYG5eQL34Y9NkArHNu8SFJCQoKko99Xbez6I0eOSDr66awk9evX74T77Nevnz766KOgjxVA57Vjxw4ZY9S/f/9Gbz/+eJAePXqcsGcjKSlJW7duDfz7tttu05tvvqns7Gz169dPEyZM0FVXXaXzzz9fklRQUKCKiopGP90+88wz5ff7tXfvXg0cODBwfVv3ynybqKioRn/boqqqKnC7k+Xm5uriiy9WQkKCXnrppRMaxGDweDyBY50uueQSjRs3Tueff75SUlJ0ySWXBKXGnXfeqeTkZN10001Bub/WqN8GQrWd7N69Ww899JCWLVvW5kY+GJ5//nklJycH9tK2tx/96Efau3evHnroIf3xj3+UJJ1zzjlasGCB7rvvvlatk6PnFVC7nxWNxgZohaYmpqaur38hA8Cx/H6/XC6X/vnPfzb6/nH8HwwteY8588wztX37dr3++utatWqV/vKXv+jxxx/XokWLtGTJEqtxNvZHY5cuXVRbW6vS0tI2fR0mPT1da9askTGmwR879Z8Gt+dpmdtbcXGxJk+erKKiIr399tsd9ljOO+88paen6/nnnw9KY7Njxw799re/1aOPPtrgq4FVVVWqqanR7t27FR8f3+oD8VsqOTlZXq83sE0cqyO2k0WLFql79+4aM2ZM4Ctoubm5ko5+ULB7925lZma26ED8ttqzZ4/efvtt3XjjjR1yIox69913n2699VZ9/vnnSkhI0ODBgwN7r04//fQW38+RI0cUHR3d7h9Y0NgAHaD+GJqdO3eecFtj1wE4OTT16eRpp50mY4z69OnTqj8OmhMTE6Pp06dr+vTp8vl8mjZtmu677z4tXLhQ3bp1U3R0tLZv335C7ssvv1RYWNgJe58bU3/8Rk5OjoYMGWI91rPOOktPPvmktm3bpgEDBgSur//+/llnnWV936FUVVWlKVOm6KuvvtKbb77Z4LF1VP3i4uKg3Nf+/fvl9/t188036+abbz7h9j59+ugnP/lJ0M6UdrywsDANHjxYH3744Qm3vf/+++rbt2+7HmuyZ88e7dy5s9Efm60/m9+RI0c65DeqXnjhBRljOuxraMdKSkrSqFGjAv9+88031aNHj1Ydy5WTk3PCiULaA8fYAB0gIyNDgwYN0jPPPNPg+8/r1q3Tp59+GsKRAWhP9T9kePxZzqZNm6bw8HAtWbLkhD27xphv/dG8phyf8Xg8GjBggIwxqqmpUXh4uCZMmKBXX321wQHQeXl5WrFihUaNGqX4+Phm69QfZ9jYH5utcemll8rtduvxxx8PXGeM0fLly9W9e3edd955bbr/UKirq9P06dO1YcMGrVy5ssExmcFUXl7e6LEKf/nLX3TkyJFGzyJmo/5nCY6/DBw4UJmZmXr55Zc1a9asoNRqyve//3198MEHDba37du366233tLll1/errXvvffeEx77L37xC0nSggUL9PLLL7fLD6E2ZsWKFcrMzGzQYITCiy++qA8++EDz5s1r1Z6qjz76qENe0+yxATrI/fffr0svvVTnn3++rr/+eh05ckSPPfaYBg0aFPSDPQF0DsOGDZMk/fznP9eMGTPkdrs1ZcoUnXbaabr33nu1cOFC7d69O3CWo5ycHL388su68cYbdeutt7aq1oQJE5SWlqbzzz9fqamp2rZtmx577DFdfPHFgU+17733Xr3xxhsaNWqUZs+erYiICP3mN79RdXV1i09l27dvXw0aNEhvvvmm/ud//qd1K+QYPXr00Lx58/TQQw+ppqZGw4cP1yuvvKK3335bzz//fIOv3z399NO6/vrr9dRTT+m6665rda2tW7fqb3/7m6Sje8mLi4t17733SpKGDh2qKVOmBJatP3udzdmvfvrTn+pvf/ubpkyZosOHD+u5555rcPuxJ4Boy2PasWOHxo8fr+nTpysrK0thYWH68MMP9dxzz6l37976yU9+0mB528fUtWtXTZ069YTr6/fQHH/b3XffrSVLlmjNmjXNnnL62Wef1TfffBNo0NavXx94Tn7wgx8Evukwe/Zs/e53v9PFF1+sW2+9VW63Ww8//LBSU1P105/+tMF9jhkzRuvWrWvR18Dra33++eeB8bzzzjuSjh5XJKnRJqJ+78zw4cNPePwul0ujR48O/L5dU7755pvAGcLqG7b68fTq1Stw9sN6n332mbZu3arbb7+9yb3Aa9eu1dixY7V48WLdfffd31p//fr1Wr9+vaSjX6krLy8P1L/wwgt14YUXBpa75557NGHCBHXp0kUbN27UU089pUmTJp2wjX2bzZs36/Dhw7r00ktbnLHW7j8BCjjUU0891eAXw3v16mUuvvjiE5ZTI7+ym5OTYySZhx56qMH1f/rTn0xWVpbxer1m0KBB5m9/+5u57LLLTFZWVrs9DgCh9Ytf/MJ0797dhIWFNXhPMcaYv/zlL2bUqFEmJibGxMTEmKysLDNnzhyzffv2wDJN/cr7zJkzTa9evQL//s1vfmMuvPBC06VLF+P1es1pp51mfvazn5ni4uIGuY8++shMnDjRxMbGmujoaDN27Fjz3nvvNVim/v3vgw8+aPQxPfzwwyY2Njbw6+nGGLNmzZoW/YL7serq6sz9999vevXqZTwejxk4cKB57rnnTlju17/+tZFkVq1a1eL7Plb942nsMnPmzAbLdu3a1Zx77rlWdUaPHt1kneP/5GrLYyooKDA33nijycrKMjExMcbj8Zj+/fubefPmmYKCghOWb8tjakxT2+RPf/pT43K5zLZt21p0H02tp+O3ob1795rvf//7Jj4+3sTGxppLLrnE7Nix44T7HDZsmElLS2vRY2jp83S8+u185cqVDa4vLS01ksyMGTOarV1/H41dRo8efcLyt99+u5Fktm7d2uR9vvbaa0aSWb58ebP1Fy9e3GT9xYsXB5bbuXOnmTBhgunatavxer0mKyvLLF261FRXVzd53439TXTbbbeZzMxM4/f7mx1bW9HYACE2dOhQM378+FAPAwBarKioyCQnJ5snn3wycF39H2uvvPKKKSgoMDU1NUGrd/nll5vhw4cH7f6a8vnnnxtJ5vXXX2/3WifjYxo+fLj5/ve/3+51GlNSUmIiIiLMY489FpL6f//7343L5frW5qM9/exnPzM9evQwVVVVIalfWFhoCgoKTmhsqqqqTFpamnn00Uc7ZBwcYwN0kJqaGtXW1ja4bu3atfrkk0+C/ivRANCeEhIStGDBAj300EPy+/0Nbps6daq6deumLVu2BKWWMUZr164NfFWmPa1Zs0YjR47UxRdf3K51TsbHVFJSok8++aRdfoi0JdavX6/u3bvrhhtuCEn9NWvWaMaMGSH7bbo1a9borrvuavQ3fzpC3759G/2do6eeekput1s/+tGPOmQcLmM4Hy3QEXbv3q3x48frmmuuUUZGhr788kstX75cCQkJ+uyzz9SlS5dQDxEArB05ckSbN28O/HvEiBEd8uvoAEJv3bp1qqmpkXT0t/0a+72sjkBjA3SQ4uJi3XjjjXr33XdVUFCgmJgYjRs3Tr/85S912mmnhXp4AAAAjkZjAwAAAMDxOMYGAAAAgOPR2AAAAABwvE73A51+v18HDhxQXFxckz9CBABoH8YYlZaWKiMjo1W/Kn2yY24CgNBozbzU6RqbAwcOqGfPnqEeBgCc0vbu3asePXqEehidBnMTAIRWS+alTtfY1J8acu66B+SNjWp1/ottB6zq7trwkVVOksLq7D+98/RMsM5+sWm3VS65S7p1zYQB4dZZb53bKnek8LB1zdzVe6yz3roiq5wvrp91zbiMSOtseLck62x0l0KrXJynq3XNyjqPdba6qtgqV+sqs65Z24a3yzCf/WMN99nlqvx1Vjnjq1Xxb1Zzmt7j/Gd9uP7v0vl1jR4W6iEAQJv5TZ0OV37Uonmp0zU29bv4vbFRVo2NO9ruD8Nwr90f3VLbGpvwSPu6Lrfd0xfmsa8ZHmXf2IRb/iEb1obnxhVuP16X5SForgj7l5XL04Y/ntuyDUfa1Q1vy7bUhsYmTHZ1w1xtWL+WNY/WbUvWLufyt+1rZHzdqqH/rA/nNDZt2d4BoLNpybzUbl+gXrZsmXr37q3IyEiNGDFCmzZtaq9SAAA0i3kJAE5u7dLYvPjii5o/f74WL16sjz76SEOHDtXEiROVn5/fHuUAAPhWzEsAcPJrl8bm4Ycf1g033KDrr79eAwYM0PLlyxUdHa0//OEP7VEOAIBvxbwEACe/oDc2Pp9Pmzdv1vjx4/9TJCxM48eP14YNG05Yvrq6WiUlJQ0uAAAES2vnJYm5CQCcKOiNzaFDh1RXV6fU1NQG16empio3N/eE5ZcuXaqEhITAhdNpAgCCqbXzksTcBABOFPJfX1u4cKGKi4sDl71794Z6SACAUxxzEwA4T9DPBdm1a1eFh4crLy+vwfV5eXlKS0s7YXmv1yuv1xvsYQAAIKn185LE3AQAThT0PTYej0fDhg3T6tWrA9f5/X6tXr1aI0eODHY5AAC+FfMSAJwa2uXXu+bPn6+ZM2fqnHPOUXZ2th599FGVl5fr+uuvb49yAAB8K+YlADj5tUtjM336dBUUFGjRokXKzc3VWWedpVWrVp1w4CYAAB2BeQkATn7t0thI0ty5czV37tz2unsAAFqFeQkATm7t1ti0VcmnZfJE17Y6t/u9D6zqHdlbZpWTpMRzzrTOHsq3P9NOzVelVrkiX411TZe7wjobVdv651OSakrtx1ubX26fLTKWyV3WNQ8f8lhnw5K7WWdj09KtcuGn2z83vpgj1lm5YqxiERFnWJcMMzuts6rz20cjo6xyblNtlfOH2Y8VAIBQCvnpngEAAACgrWhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAABwvItQDaErO19vljvS0OpfkT7eq50mvtcpJkrugxDpbvDPfOqvKGquY79AB65KFxZXW2fBoy82txG9dU/ty7bOxkZbBePua+QnWUX/ZQetsSX6ZXc2oJOuaXfq5rLOqtqsbIfvXW5W7yjobFm3/HmE8UVa5qhK77dD42/C8oFPJL99knU2JyQ7iSACgY7DHBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHiwj1AJrirYuXu87b6lyXuBireqWxcVY5SUrq7bHOJoR1sc56XQVWuUpzwLpmXW2ZdfbQ15VWubDqQuua7sRI62yNz7Lvr8izrqkSY59tw8cU4akVVjmTn2pdMyLN/rkJ95RY5o5Y16yosX+PiA7raZ2t9dg9Ny5XuVXO76pVsVUSJ5P88k2hHkKHSYnJDvUQTmqh2JZ4Tk9d7LEBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPEiQj2ApoTHG4VHmlbnIqJ8VvV6psVa5STJ1SfKOlsb3s86+1/9vmOVKwnbZV1z38FS6+wXFZ9b5dwe+/473JVknd2zodIuWLvPuqaSa+yztfYv556DM61yJizOumbxFy7rbHzfOrugO926pjemzDobZvzWWXdYlVXO37PcLldZa5VD+6j1/zHUQ2i1iLCZoR5Cq+SXbwr1EBBkbXlOU2KygzgSdDT22AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeBGhHkBTYjJPlyc6qtW5Xl2TrerFxnqtcpJ0aF+lddZX67fOpvROt8vFx1rXDIs6ZJ11HSixy9XaPU5Jqgl3WWcP7tlmV9N1pnVNJX5tny08Yh11uWuscl1c9p+NlOTlWGcrXHavc4+v9e8p9cJPs3+snjpjnQ1zR1rlfO46q5yrttYqh29X6/9jqIfQYU6VxxoRNjPUQwBwHPbYAAAAAHA8GhsAAAAAjkdjAwAAAMDxgt7Y3H333XK5XA0uWVlZwS4DAECLMTcBwMmvXU4eMHDgQL355pv/KRLRac9RAAA4RTA3AcDJrV3e1SMiIpSWltYedw0AgBXmJgA4ubXLMTY7duxQRkaG+vbtq6uvvlp79uxpctnq6mqVlJQ0uAAAEGzMTQBwcgt6YzNixAg9/fTTWrVqlZ544gnl5OToggsuUGlpaaPLL126VAkJCYFLz549gz0kAMApjrkJAE5+QW9sJk+erMsvv1xDhgzRxIkT9Y9//ENFRUX685//3OjyCxcuVHFxceCyd+/eYA8JAHCKY24CgJNfux85mZiYqNNPP107d+5s9Hav1yuv19vewwAAIIC5CQBOPu3+OzZlZWXatWuX0tPT27sUAAAtwtwEACefoDc2t956q9atW6fdu3frvffe0/e+9z2Fh4fryiuvDHYpAABahLkJAE5+Qf8q2r59+3TllVeqsLBQ3bp106hRo7Rx40Z169Yt2KUAAGgR5iYAOPkFvbH505/+FJT7GTxggCLjYlqdS6j1WdXzHSm3yklSxYE462yYz75uenKRVc5Vaj+Rl0e4rLP7PY2ffag5sfmZ1jWL/GXW2Ujtt8q5uyZb16yIjbTOumP81tnSIzVWudioWuuaiUldrbNHShOtclUFH1vXjM040zrrjYuyzla5o61y7hq713ldjd17aGcXrLnpcNFyxcfbPScAOr/88k3W2ZSY7CCOBDba/RgbAAAAAGhvNDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxIkI9gKakeDyK9nhbnasr9lvVq6xMtspJkr92t3U2ubbIOhu9P8sqF5daa12zKOywdbakqMQqFxFVZ12zoOAz66zHa7dN+BOtS6r28CHrrDs1zTob4elilSs9st+6psduc5AkdfPUWOWO1BZb1yx+a5t1tvfEi62zFRGWb9PeArtcrc8uBwCnuPzyTVa5lJjsII/k1MUeGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcLyLUA2iKpyRKHn9Uq3PlpdVW9SryD1vlJKkuzD7rqrWOqtrlt8qleWKsa8YV2A/48K4iq5zfu8O6ZsHhAutsclSmXU3fAeuaLqVZZ6sioq2zkTV2ubpo++2hx3cuss5GKNYql/uF3fuDJJUdzrfOfrUlxzobOSzeKhcdaewKmjq7HADASn75JutsSkx2EEfifOyxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjhcR6gE0xVddoQh36/uuw4cqrOoV5eVa5STJG5ZgnY0Os+8tE9NdlrkY65omN9I6W1NprHK7juywrlm0r9o66+tmt034wt3WNaPS6qyzRZV2274keWprrHKJSRnWNU/rPsQ6+52+/a1yqRXJ1jU3HnjXOltT8o11tiq31C5o7LZDf1WtXT0AQIfLL99knU2JyQ7iSDoH9tgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4HgRoR5AUyoqDsu4qludyy/MtarnO1xqlZOk6IRE62yXhDzrbN/Y7la5zKSu1jVzwuOss3HuVKvcngMF1jVVZx8tPmy3LSn2sHVNV1U362xMebJ1Ni7T7q0gNSvLuubOqK+ss5ERVVa5uPPsx5u08WPrbGV5hnW26JMKq9zhuEKrnKlpw4sGAOAY+eWbrLMpMdlBHEnwsMcGAAAAgOPR2AAAAABwPBobAAAAAI7X6sZm/fr1mjJlijIyMuRyufTKK680uN0Yo0WLFik9PV1RUVEaP368duzYEazxAgDQAPMSAECyaGzKy8s1dOhQLVu2rNHbH3zwQf3qV7/S8uXL9f777ysmJkYTJ05UVZXdwb4AAHwb5iUAgGRxVrTJkydr8uTJjd5mjNGjjz6qO++8U5deeqkk6ZlnnlFqaqpeeeUVzZgxo22jBQDgOMxLAAApyMfY5OTkKDc3V+PHjw9cl5CQoBEjRmjDhg2NZqqrq1VSUtLgAgBAMNjMSxJzEwA4UVAbm9zco7/7kZra8PdKUlNTA7cdb+nSpUpISAhcevbsGcwhAQBOYTbzksTcBABOFPKzoi1cuFDFxcWBy969e0M9JADAKY65CQCcJ6iNTVpamiQpLy+vwfV5eXmB247n9XoVHx/f4AIAQDDYzEsScxMAOFFQG5s+ffooLS1Nq1evDlxXUlKi999/XyNHjgxmKQAAmsW8BACnjlafFa2srEw7d+4M/DsnJ0dbtmxRcnKyMjMzNW/ePN17773q37+/+vTpo7vuuksZGRmaOnVqMMcNAIAk5iUAwFGtbmw+/PBDjR07NvDv+fPnS5Jmzpypp59+WgsWLFB5ebluvPFGFRUVadSoUVq1apUiIyODN2oAAP4P8xIAQLJobMaMGSNjTJO3u1wu3XPPPbrnnnvaNDAAAFqCeQkAIFk0Nh2lqKpUVeF1rc4V5Byyqhcf4bPKSVJidOvHWS85doB1tmdKP6tckiu1+YWakJm4yzqbFNfDKhfjPWhd00S4rbMVxcV2wZoo+5olCdbZ6FiPddZXa7cNl5tq65oVSeXW2bdyPrfKhW2Ls67pL6q0zp5+QTfrbPF+u+3/441272mmzv79DABwasgv32SVS4nJDvJIGgr56Z4BAAAAoK1obAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcLyLUA2hKmYlWjYlpda4kzGdVL7y2zionSUnecutsTJzdeCUptrrMKheWkG5fc4D9Y01KtcumHIi1rukLS7HORodXWOWqwt3WNWs9fuusN6rEOuuqtVtP+QWHrGt6cuzffooP2j03pUV2rxlJivVHWWfd8V2ss/27nWmV8xVus8rV1fi0efPnVlkAAEKJPTYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOF5EqAfQlASXX5GuulbnUpLSreolFuZb5SQpPjzNOmsiu1hni4srrXLhvaxLqtI3xDqbkfWuVa6q3GVdMzk8xTpbFOGxypVG2G9L2w9XW2djq/3W2dMzvVa5itpo65qfbTpsnT3iK7bKJbnt1296ZnfrbEVRrXU2IbLcKjdgWIZVzldVrc1/tooCjpQRtyzUQwBCIiUmO9RDCDr22AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeBGhHkBTwhLCFBYb3upcZkaaVb3EiN1WOUmqivZYZysiyq2zeyOqrXLf7N1gXfPLyjzr7KGkeKtcl4xo65rdUu03cZPrssp5/MnWNeN8n1tnZezWrySVltrlimv3WNf0FVRZZ72JxVa5tB79rGsOvGiUdbbYZbmCJRWV2L2/+MLdVrkal912D4RSRtyyUA8BCImUmOxQD6FTYY8NAAAAAMejsQEAAADgeDQ2AAAAAByPxgYAAACA49HYAAAAAHA8GhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI4XEeoBNKUu3q26WHerc0ldoqzqheePs8pJUnFEkXW2NtZlnS2NjrPK7ffss675jWePdfagv8YqNzjWWNc0YV2ts97qr61ycdHdrGtmRqRbZ3PDi6yz3+wps8rlR+Ra14wwkdbZ9IguVrl+WdnWNdNOT7POxuyoss767F42OpR72Crnqq62Kwi0UUbcslAPoVVSYuzfT/LLNwVxJADqsccGAAAAgOPR2AAAAABwvFY3NuvXr9eUKVOUkZEhl8ulV155pcHt1113nVwuV4PLpEmTgjVeAAAaYF4CAEgWjU15ebmGDh2qZcua/i7spEmTdPDgwcDlhRdeaNMgAQBoCvMSAECyOHnA5MmTNXny5G9dxuv1Ki3N/kBbAABainkJACC10zE2a9euVUpKis444wz9+Mc/VmFhYXuUAQCgRZiXAODkF/TTPU+aNEnTpk1Tnz59tGvXLt1xxx2aPHmyNmzYoPDw8BOWr66uVvUxpxctKSkJ9pAAAKew1s5LEnMTADhR0BubGTNmBP5/8ODBGjJkiE477TStXbtW48ad+FsxS5cu1ZIlS4I9DAAAJLV+XpKYmwDAidr9dM99+/ZV165dtXPnzkZvX7hwoYqLiwOXvXv3tveQAACnsObmJYm5CQCcKOh7bI63b98+FRYWKj298V9U93q98nq97T0MAAAkNT8vScxNAOBErW5sysrKGnzKlZOToy1btig5OVnJyclasmSJLrvsMqWlpWnXrl1asGCB+vXrp4kTJwZ14AAASMxLAICjWt3YfPjhhxo7dmzg3/Pnz5ckzZw5U0888YS2bt2qP/7xjyoqKlJGRoYmTJigX/ziF3zyBQBoF8xLAADJorEZM2aMjDFN3v6vf/2rTQMCAKA1mJcAAFIHHGNjK6IsVhGKbXWuPKncql7SmclWOUkqDWv6ANTm7Nha3fxCTeg5padV7kBEvnXNb760P9+E/3CBVa40wv5H9XrF+a2zZRlRVjmfx21dMzWhm3W2t7eXdbbSV2aVy//abhuUpPLyCuusJ9LurSs10n77jXU1flrglnCnJlhny2S3PRVV2b3OTXidVQ4nl4y4ZaEeAqCxUT/s8JprKp/s8JopMdkdXvNk1e5nRQMAAACA9kZjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHC8i1ANoiglzyYS5Wp1L7BNtVS8sLNIqJ0kVhzzW2YNffG2dzS9OtsodqXRb1/zmo1zrbMnnOVY5f7z9ZhpVZB1VYW2ZVS62t33NxPSe1tmMmETrbF5YnlUu7vQ+1jU9rmLrbGGVXTanaI91zYObyq2zXdNjrLMpPezem8JqDtjlKmuscmgfGXHLQj0EdDIpMdkdXjO/fJN1dmzUD4M4kvYXivF+rq0dXvNkxR4bAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAABwvItQDaErB5gPyRkW3OlfXpcqqXljPnlY5STpc4bfO5iYfts7mbVhjlXMdclnXrHh3r3W2fP9+q1x4RIJ1za/qyq2zlTGlVrk0b7h1Te0rtI5+HVZnna2LtKtbG19pXTPaa/+5SkJClFUu/LDd+4Mk5a+xf25yuuRZZ8tPt8v5IntZ5Woqq+0KniKSE38kqfXvoSkx2cEfDEIqv3yTdTYU28NA/xD7bJR9Fs1ry3PzedjWII7E+dhjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHC8i1ANoyhfvr5Pb42l1zqc0q3qxF/mtcpJUVPW5dTZ3Z411tnDHYauc+/Ae65pHckuss10Teljl4qO6WteU+5B11K9oq1zp3ljrmpE1ds+pJPkjSq2zFcay5m7LoCR/uts6Gxlv99wkdMmwrhl7nv22VFFh/9wc3mv3+VNNzW6rXK3P/j0JCJX88k3W2ZSY7A7NtdVA/5CQ1EXnFKrtYU3lk1a59n7dsMcGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMeLCPUAmvJVfo7CItytzh350xGreu61X1rlJCksstg6W7ujyjob0y3BKldxONW6Ztc4r3U2Na2bVS4iYp91zYoqn3U2Lqa7VS7K57eu6fIXWGdrPGnWWeOyG3NYjP32q6Iu1tFyY1e3oq7auqY7zO71JknxqSnW2W5Jdq+5wwV5Vjlftf06AiQpv3xTh9dMicnu8JptMdA/JNRDANpkbNQP7YIWf27UGp/e1gctWpY9NgAAAAAcj8YGAAAAgOPR2AAAAABwvFY1NkuXLtXw4cMVFxenlJQUTZ06Vdu3b2+wTFVVlebMmaMuXbooNjZWl112mfLy7L7rDQBAc5ibAABSKxubdevWac6cOdq4caPeeOMN1dTUaMKECSovLw8sc8stt+i1117TypUrtW7dOh04cEDTpk0L+sABAJCYmwAAR7XqrGirVq1q8O+nn35aKSkp2rx5sy688EIVFxfr97//vVasWKGLLrpIkvTUU0/pzDPP1MaNG3XuuecGb+QAAIi5CQBwVJuOsSkuPnqa4+TkZEnS5s2bVVNTo/HjxweWycrKUmZmpjZs2NDofVRXV6ukpKTBBQAAW8xNAHBqsm5s/H6/5s2bp/PPP1+DBg2SJOXm5srj8SgxMbHBsqmpqcrNzW30fpYuXaqEhITApWfPnrZDAgCc4pibAODUZd3YzJkzR5999pn+9Kc/tWkACxcuVHFxceCyd+/eNt0fAODUxdwEAKeuVh1jU2/u3Ll6/fXXtX79evXo0SNwfVpamnw+n4qKihp8MpaXl6e0tMZ/Fd3r9crrtf81ewAAJOYmADjVtWqPjTFGc+fO1csvv6y33npLffr0aXD7sGHD5Ha7tXr16sB127dv1549ezRy5MjgjBgAgGMwNwEApFbusZkzZ45WrFihV199VXFxcYHvJickJCgqKkoJCQmaNWuW5s+fr+TkZMXHx+umm27SyJEjOesMAKBdMDcBAKRWNjZPPPGEJGnMmDENrn/qqad03XXXSZIeeeQRhYWF6bLLLlN1dbUmTpyoxx9/PCiDBQDgeMxNAACplY2NMabZZSIjI7Vs2TItW7bMelAAALQUcxMAQLI8eUBHKMyRXOGtzxX7P7WqZ772WeWOhuOto12Sm5+Qm9L9zO5WOXdYrXXNIpU3v1ATSmrssonuOuuaCd7GDwxuifikKMuk/csqrGqwddYdbfGCqc/6qqxy4d5S65qeKI91ttpXaZWr2Wf/WyTVnnTrbE1qsXVW3mirWM+0TKtcdaXdukXnk1++KSR1U2KyQ1K3ow30Dwn1EAAcp00/0AkAAAAAnQGNDQAAAADHo7EBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcLyIUA+gKRWHd8rlan3fZWLKrepFxSVa5SSpR3Qb+sNw++yhb/balfRal5TCo6yjNVGxVjlvG56btKgY62x0tN3Lo9bvt67pi0q2zia7KqyzLletVa62rKt1TV+q/WOt8x+xyiXE+qxrVplq62xdZZl19khlmlXO5Tlolauusn+caFp++SarXEpMtnXNtmQBwInYYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAABwvItQDaEq0t6dcYa0fXmx0nlU9b7TXKidJrm7GOuurKLbOVh6xG3NEWqx1zYTUDOtsD5fbKpcWbT9ed5R1VN4wv1Uu1l1tXbOs3G4dSVK1q9Y6G53issp5Y/tZ14zwVFpn67x2r/OIOvttKboq2jpr/D2ss+4Ej11NFdnlXD6rHOBUA/1DQj0EAEHCHhsAAAAAjkdjAwAAAMDxaGwAAAAAOB6NDQAAAADHo7EBAAAA4Hg0NgAAAAAcj8YGAAAAgOPR2AAAAABwPBobAAAAAI5HYwMAAADA8WhsAAAAADgejQ0AAAAAx6OxAQAAAOB4EaEewPGMMUf/66+1y9fVWeX8ljlJqqu17w/9tfZ1/XV268hfU2Nds87ns87WuoxVzldVbV3TuKyjcoXZhf3h9uP1VVVaZ12uKutseKXdduhqw3j9dfZZn+zWscvvt67pr7J/nRu//XNjPHbPjZHda9VXdTRX/16Mo/6zPjp2vfiN3fs8Wq7W2M9rANpf/Wu0JfNSp2tsSktLJUklBRvs7iAviIPpvCX/z/aQVbbxWagHAKDFSktLlZCQEOphdBr1c9PRxqbjmptDFR90WK1T1dtiHQNO0JJ5yWU62cdyfr9fBw4cUFxcnFyuEz8xLykpUc+ePbV3717Fx8eHYISdH+uoeayjlmE9Ne9kW0fGGJWWliojI0NhYXxbud63zU0n2zbQXlhPzWMdtQzrqXkn0zpqzbzU6fbYhIWFqUePHs0uFx8f7/gnqr2xjprHOmoZ1lPzTqZ1xJ6aE7VkbjqZtoH2xHpqHuuoZVhPzTtZ1lFL5yU+jgMAAADgeDQ2AAAAABzPcY2N1+vV4sWL5fV6Qz2UTot11DzWUcuwnprHOgLbQMuwnprHOmoZ1lPzTtV11OlOHgAAAAAAreW4PTYAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4nqMam2XLlql3796KjIzUiBEjtGnTplAPqVO5++675XK5GlyysrJCPayQWr9+vaZMmaKMjAy5XC698sorDW43xmjRokVKT09XVFSUxo8frx07doRmsCHU3Hq67rrrTti2Jk2aFJrBhsDSpUs1fPhwxcXFKSUlRVOnTtX27dsbLFNVVaU5c+aoS5cuio2N1WWXXaa8vLwQjRgdibmpacxLjWNuah7zUvOYm07kmMbmxRdf1Pz587V48WJ99NFHGjp0qCZOnKj8/PxQD61TGThwoA4ePBi4vPPOO6EeUkiVl5dr6NChWrZsWaO3P/jgg/rVr36l5cuX6/3331dMTIwmTpyoqqqqDh5paDW3niRp0qRJDbatF154oQNHGFrr1q3TnDlztHHjRr3xxhuqqanRhAkTVF5eHljmlltu0WuvvaaVK1dq3bp1OnDggKZNmxbCUaMjMDc1j3npRMxNzWNeah5zUyOMQ2RnZ5s5c+YE/l1XV2cyMjLM0qVLQziqzmXx4sVm6NChoR5GpyXJvPzyy4F/+/1+k5aWZh566KHAdUVFRcbr9ZoXXnghBCPsHI5fT8YYM3PmTHPppZeGZDydUX5+vpFk1q1bZ4w5ut243W6zcuXKwDLbtm0zksyGDRtCNUx0AOamb8e81DzmpuYxL7UMc5Mxjthj4/P5tHnzZo0fPz5wXVhYmMaPH68NGzaEcGSdz44dO5SRkaG+ffvq6quv1p49e0I9pE4rJydHubm5DbarhIQEjRgxgu2qEWvXrlVKSorOOOMM/fjHP1ZhYWGohxQyxcXFkqTk5GRJ0ubNm1VTU9NgW8rKylJmZibb0kmMuallmJdah7mp5ZiXGmJucshX0Q4dOqS6ujqlpqY2uD41NVW5ubkhGlXnM2LECD399NNatWqVnnjiCeXk5OiCCy5QaWlpqIfWKdVvO2xXzZs0aZKeeeYZrV69Wg888IDWrVunyZMnq66uLtRD63B+v1/z5s3T+eefr0GDBkk6ui15PB4lJiY2WJZt6eTG3NQ85qXWY25qGealhpibjooI9QAQPJMnTw78/5AhQzRixAj16tVLf/7znzVr1qwQjgxON2PGjMD/Dx48WEOGDNFpp52mtWvXaty4cSEcWcebM2eOPvvsM44TAFqAeQnthXmpIeamoxyxx6Zr164KDw8/4SwOeXl5SktLC9GoOr/ExESdfvrp2rlzZ6iH0inVbztsV63Xt29fde3a9ZTbtubOnavXX39da9asUY8ePQLXp6WlyefzqaioqMHybEsnN+am1mNeah5zk51TdV6SmJuO5YjGxuPxaNiwYVq9enXgOr/fr9WrV2vkyJEhHFnnVlZWpl27dik9PT3UQ+mU+vTpo7S0tAbbVUlJid5//322q2bs27dPhYWFp8y2ZYzR3Llz9fLLL+utt95Snz59Gtw+bNgwud3uBtvS9u3btWfPHralkxhzU+sxLzWPucnOqTYvScxNjXHMV9Hmz5+vmTNn6pxzzlF2drYeffRRlZeX6/rrrw/10DqNW2+9VVOmTFGvXr104MABLV68WOHh4bryyitDPbSQKSsra/DpTU5OjrZs2aLk5GRlZmZq3rx5uvfee9W/f3/16dNHd911lzIyMjR16tTQDToEvm09JScna8mSJbrsssuUlpamXbt2acGCBerXr58mTpwYwlF3nDlz5mjFihV69dVXFRcXF/huckJCgqKiopSQkKBZs2Zp/vz5Sk5OVnx8vG666SaNHDlS5557bohHj/bE3PTtmJcax9zUPOal5jE3NSLUp2VrjV//+tcmMzPTeDwek52dbTZu3BjqIXUq06dPN+np6cbj8Zju3bub6dOnm507d4Z6WCG1Zs0aI+mEy8yZM40xR0+redddd5nU1FTj9XrNuHHjzPbt20M76BD4tvVUUVFhJkyYYLp162bcbrfp1auXueGGG0xubm6oh91hGls3ksxTTz0VWKaystLMnj3bJCUlmejoaPO9733PHDx4MHSDRodhbmoa81LjmJuax7zUPOamE7mMMab92ycAAAAAaD+OOMYGAAAAAL4NjQ0AAAAAx6OxAQAAAOB4NDYAAAAAHI/GBgAAAIDj0dgAAAAAcDwaGwAAAACOR2MDAAAAwPFobAAAAAA4Ho0NAAAAAMejsQEAAADgeDQ2AAAAABzv/wOb3vajD2mpcQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(train_loader))\n",
    "\n",
    "fix, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'{label.unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Vision Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the linear layer. If ``None`` this layer won't be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the linear layer. If ``None`` this layer won't be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool, optional): Parameter for the activation layer, which can optionally do the operation in-place.\n",
    "            Default is ``None``, which uses the respective default values of the ``activation_layer`` and Dropout layer.\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = None,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        # self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        # input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSat Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, img_height=24, img_width=24, in_channel=10,\n",
    "                       patch_size=3, embed_dim=128, max_time=60,\n",
    "                       num_classes=20, num_head=4, dim_feedforward=2048,\n",
    "                       num_layers=4\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H = img_height\n",
    "        self.W = img_width\n",
    "        self.P = patch_size\n",
    "        self.C = in_channel\n",
    "        self.d = embed_dim\n",
    "        self.T = max_time\n",
    "        self.K = num_classes\n",
    "\n",
    "        self.d_model = self.d\n",
    "        self.num_head = num_head\n",
    "        self.dim_feedforward = self.d\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.N = int(self.H * self.W // self.P**2)\n",
    "        self.nh = int(self.H / self.P)\n",
    "        self.nw = int(self.W / self.P)\n",
    "\n",
    "\n",
    "        '''\n",
    "        PARAMETERS\n",
    "        '''\n",
    "        # Transformer Encoder\n",
    "\n",
    "        # PyTorch Encoder\n",
    "        # self.encoderLayer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=self.num_head, dim_feedforward=self.dim_feedforward)\n",
    "        # self.encoder = nn.TransformerEncoder(self.encoderLayer, num_layers=self.num_layers)\n",
    "\n",
    "        # DeepSat Encoder\n",
    "        self.encoder = Transformer(self.d, self.num_layers, self.num_head, 32, self.d*4)\n",
    "\n",
    "        # torchvision Encoder\n",
    "        # self.encoder = Encoder(seq_length=self.N, num_heads=4, num_layers=4, hidden_dim=self.d, mlp_dim=self.d*4, dropout=0., attention_dropout=0.)\n",
    "\n",
    "        # Patches\n",
    "        self.projection = nn.Conv3d(self.C, self.d, kernel_size=(1, self.P, self.P), stride=(1, self.P, self.P))\n",
    "        '''\n",
    "        def __init__():\n",
    "            self.linear = nn.Linear(self.C*self.P**2, self.d)\n",
    "        def forward():\n",
    "            x = x.view(B, T, H // P, W // P, C*P**2)\n",
    "            x = self.linear(x)\n",
    "        '''\n",
    "\n",
    "        # Temporal\n",
    "        self.temporal_emb = nn.Linear(366, self.d)\n",
    "        self.temporal_cls_token = nn.Parameter(torch.randn(1, self.N, self.K, self.d)) # (N, K, d)\n",
    "        self.temporal_transformer = self.encoder\n",
    "\n",
    "        # Spatial\n",
    "        self.spatial_emb = nn.Parameter(torch.randn(1, self.N, self.d)) # (1, N, d)\n",
    "        # self.spatial_cls_token = nn.Parameter(torch.randn(1, self.K, self.d)) # (1, K, d)\n",
    "        self.spatial_transformer = self.encoder\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.d),\n",
    "            nn.Linear(self.d, self.P**2)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Tekenization\n",
    "\n",
    "        Convert the images to a sequence of patches\n",
    "        '''\n",
    "        x_sits = x[:, :, :-1, :, :] # (B, T, C, H, W) -- > Exclude DOY Channel\n",
    "        B, T, C, H, W = x_sits.shape # (B, T, C, H, W)\n",
    "        x_sits = x_sits.reshape(B, C, T, H, W) # (B, C, T, H, W)\n",
    "        x_sits = self.projection(x_sits) # (B, d, T, nw, nh)\n",
    "        x_sits = x_sits.reshape(B, self.d, T, self.nh*self.nw) # (B, d, T, N)\n",
    "        # x_sits = x_sits + self.pos_emb # (B, d, T, N)  we dont add pos embedding here, cuz we need the pure data for the temporal encoder\n",
    "        x_sits = x_sits.permute(0,3,2,1) # (B, N, T, d)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoding\n",
    "\n",
    "        (DOY -> One-Hot -> Projection)\n",
    "        '''\n",
    "        xt = x[:, :, -1, 0, 0] # (B, T, C, H, W) in the last channel lies the DOY feature\n",
    "        xt = F.one_hot(xt.to(torch.int64), num_classes=366).to(torch.float32) # (B, T, 366)\n",
    "        Pt = self.temporal_emb(xt) # (B, T, d) (DOY, one-hot encoded to represent the DOY feature and then encoded to d dimensions)\n",
    "\n",
    "        '''\n",
    "        Temporal Encoder: cat(Z+Pt)\n",
    "\n",
    "        add temporal embeddings (N*K) to the Time Series patches (T)\n",
    "        '''\n",
    "        x = x_sits + Pt.unsqueeze(1) # (B, N, T, d)\n",
    "        temporal_cls_token = self.temporal_cls_token # (1, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.repeat(B, 1, 1, 1) # (B, N, K, d)\n",
    "        temporal_cls_token = temporal_cls_token.reshape(B*self.N, self.K, self.d) # (B*N, K, d)\n",
    "        x = x.reshape(B*self.N, T, self.d) # (B*N, T, d)\n",
    "        # Temporal Tokens (N*K)\n",
    "        x = torch.cat([temporal_cls_token, x], dim=1) # (B*N, K+T, d)\n",
    "        # Temporal Transformer\n",
    "        x = self.temporal_transformer(x) # (B*N, K+T, d)\n",
    "        x = x.reshape(B, self.N, self.K + T, self.d) # (B, N, K+T, d)\n",
    "        x = x[:,:,:self.K,:] # (B, N, K, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, K, N, d)\n",
    "        x = x.reshape(B*(self.K), self.N, self.d) # (B*K, N, d)\n",
    "\n",
    "        '''\n",
    "        Spatial Encoding\n",
    "        '''\n",
    "        Ps = self.spatial_emb # (1, N, d)\n",
    "        x = x + Ps # (B*K, N, d)\n",
    "        '''\n",
    "        # For Classification Only\n",
    "        # spatial_cls_token = self.spatial_cls_token # (1, K, d)\n",
    "        # spatial_cls_token = spatial_cls_token.unsqueeze(2) # (1, K, 1, d)\n",
    "        # spatial_cls_token = spatial_cls_token.repeat(B, 1, 1, 1) # (B, K, 1, d)\n",
    "        # x = torch.cat([spatial_cls_token, x], dim=2) # (B, K, 1+N, d)\n",
    "        '''\n",
    "        x = self.spatial_transformer(x) # (B*K, N, d)\n",
    "        x = x.reshape(B, self.K, self.N, self.d) # (B, K, N, d)\n",
    "        x = x.permute(0, 2, 1, 3) # (B, N, K, d)\n",
    "\n",
    "        '''\n",
    "        Segmentation Head\n",
    "        '''\n",
    "        # classes = x[:,:,0,:] # (B, K, d)\n",
    "        # x = x[:,:,1:,:] # (B, K, N, d)\n",
    "        \n",
    "        x = self.mlp_head(x) # (B, N, K, P*P)\n",
    "\n",
    "        '''\n",
    "        Reassemble\n",
    "        '''\n",
    "        x = x.permute(0, 2, 3, 1) # (B, N, P*P, K)\n",
    "        x = x.reshape(B, self.N, self.P, self.P, self.K) # (B, N, P, P, K)\n",
    "        x = x.reshape(B, self.H, self.W, self.K) # (B, H, W, K)\n",
    "        # x = x.permute(0, 3, 1, 2) # (B, K, H, W)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, mean=True):\n",
    "        super(MaskedCrossEntropyLoss, self).__init__()\n",
    "        self.mean = mean\n",
    "    \n",
    "    def forward(self, logits, ground_truth):\n",
    "        if type(ground_truth) == torch.Tensor:\n",
    "            target = ground_truth\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 1:\n",
    "            target = ground_truth[0]\n",
    "            mask = None\n",
    "        elif len(ground_truth) == 2:\n",
    "            target, mask = ground_truth\n",
    "        else:\n",
    "            raise ValueError(\"ground_truth parameter for MaskedCrossEntropyLoss is either (target, mask) or (target)\")\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask_flat = mask.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            nclasses = logits.shape[-1]\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_logits_flat = logits_flat[mask_flat.repeat(1, nclasses)].view(-1, nclasses)\n",
    "            target_flat = target.reshape(-1, 1)  # (N*H*W x 1)\n",
    "            masked_target_flat = target_flat[mask_flat].unsqueeze(dim=-1).to(torch.int64)\n",
    "        else:\n",
    "            masked_logits_flat = logits.reshape(-1, logits.size(-1))  # (N*H*W x Nclasses)\n",
    "            masked_target_flat = target.reshape(-1, 1).to(torch.int64)  # (N*H*W x 1)\n",
    "        masked_log_probs_flat = torch.nn.functional.log_softmax(masked_logits_flat, dim=1)  # (N*H*W x Nclasses)\n",
    "        masked_losses_flat = -torch.gather(masked_log_probs_flat, dim=1, index=masked_target_flat)  # (N*H*W x 1)\n",
    "        if self.mean:\n",
    "            return masked_losses_flat.mean()\n",
    "        return masked_losses_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  1023881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Segmentation(\n",
       "  (encoder): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Conv3d(10, 128, kernel_size=(1, 3, 3), stride=(1, 3, 3))\n",
       "  (temporal_emb): Linear(in_features=366, out_features=128, bias=True)\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (spatial_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (4): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Data\n",
    "batch_size = 8\n",
    "train_set, val_set = random_split(data, [400, 100])\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=True)\n",
    "num_samples = train_loader.__len__()*batch_size\n",
    "\n",
    "# Model\n",
    "model = Segmentation(img_width=24, img_height=24, in_channel=10, patch_size=3, embed_dim=128, max_time=60, num_head=4, num_layers=4, num_classes=20)\n",
    "model.to(device)\n",
    "\n",
    "num_params = sum([p.numel() for p in model.parameters() if p.requires_grad == True])\n",
    "print('Number of Parameters: ', num_params)\n",
    "\n",
    "# Loss\n",
    "criterion = MaskedCrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",

    "# optimizer = optim.SGD(model.parameters(), lr=5e-2, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "\n",
    "  t1 = time.time()\n",
    "  for batch in tqdm(train_loader):\n",
    "    img, label = batch\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    # print(f'Output shape: {output.shape} | Label shape: {label.shape}')\n",
    "    # print('Output: ', output[0], 'Label: ', label[0])\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    epoch_loss += loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    torch.save({\n",
    "              'epoch': epoch,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': loss,\n",
    "              }, f'/workspace/weights/epoch_{epoch}.pt')\n",
    "  t2 = time.time()\n",
    "\n",
    "  \n",
    "    \n",
    "  print('Epoch: ', epoch, 'Loss: ', (epoch_loss/num_samples_train)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training milad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.metrics import Loss\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, WeightsScalarHandler, WeightsHistHandler, GradsScalarHandler\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.handlers import LRScheduler\n",
    "from ignite.metrics import RunningAverage\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Metric\n",
    "from ignite.metrics.metric import sync_all_reduce, reinit__is_reduced\n",
    "\n",
    "class CustomAccuracy(Metric):\n",
    "    def __init__(self, output_transform=lambda x: x):\n",
    "        super(CustomAccuracy, self).__init__(output_transform=output_transform)\n",
    "\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self._num_correct = 0\n",
    "        self._num_examples = 0\n",
    "\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        predicted_classes = torch.argmax(y_pred, dim=-1)\n",
    "        correct = (predicted_classes == y).float().sum().item()\n",
    "        self._num_correct += correct\n",
    "        self._num_examples += y.numel()\n",
    "\n",
    "    @sync_all_reduce(\"_num_correct\", \"_num_examples\")\n",
    "    def compute(self):\n",
    "        return self._num_correct / self._num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model  # Your model\n",
    "train_loader = train_loader  # Your training data loader\n",
    "val_loader = train_loader  # Your validation data loader\n",
    "test_loader = train_loader  # Your test data loader\n",
    "optimizer = optimizer  # Your optimizer\n",
    "criterion = criterion  # Your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.getLogger('ignite.engine.engine.Engine').setLevel(logging.WARNING)  # <-- Set logging level to WARNING\n",
    "\n",
    "def setup_directories(log_dir, model_save_dir):\n",
    "    \"\"\"Setup directories if they don't exist.\"\"\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    if not os.path.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, max_epochs=100, log_dir='./tb_logs', model_save_dir='./checkpoints'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Create trainer and evaluator\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    evaluator = create_supervised_evaluator(model, metrics={\"accuracy\": CustomAccuracy(), \"loss\": Loss(criterion)}, device=device)\n",
    "\n",
    "    # Tensorboard Logger\n",
    "    tb_logger = TensorboardLogger(log_dir=log_dir)\n",
    "\n",
    "    # Attach various logging handlers to Tensorboard for training\n",
    "    tb_logger.attach(trainer, log_handler=OutputHandler(tag=\"training\", output_transform=lambda loss: {'loss': loss}),\n",
    "                     event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=WeightsScalarHandler(model), event_name=Events.ITERATION_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=WeightsHistHandler(model), event_name=Events.EPOCH_COMPLETED)\n",
    "    tb_logger.attach(trainer, log_handler=GradsScalarHandler(model), event_name=Events.ITERATION_COMPLETED)\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    scheduler = LRScheduler(lr_scheduler)\n",
    "    trainer.add_event_handler(Events.ITERATION_COMPLETED, scheduler)\n",
    "\n",
    "    # Model Checkpointing\n",
    "    checkpoint_handler = ModelCheckpoint(model_save_dir, 'model', n_saved=3, require_empty=False)\n",
    "    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': model})\n",
    "\n",
    "    # Early Stopping\n",
    "    def score_function(engine):\n",
    "        val_loss = engine.state.metrics['loss']\n",
    "        return -val_loss\n",
    "\n",
    "    handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n",
    "    evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "    # Save the first model after the first epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(once=1))\n",
    "    def save_first_model(engine):\n",
    "        epoch = engine.state.epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'first_model_epoch_{epoch}.pth'))\n",
    "\n",
    "    # Save the last model after each epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def save_last_model(engine):\n",
    "        epoch = engine.state.epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'last_model_epoch_{epoch}.pth'))\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    best_accuracy = 0.0\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def save_best_model(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        accuracy = evaluator.state.metrics['accuracy']\n",
    "        nonlocal best_accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            epoch = engine.state.epoch\n",
    "            torch.save(model.state_dict(), os.path.join(model_save_dir, f'best_model_epoch_{epoch}.pth'))\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    # Log training loss after each iteration\n",
    "    @trainer.on(Events.ITERATION_COMPLETED)\n",
    "    def log_training_iteration(engine):\n",
    "        iteration = (engine.state.iteration - 1) % len(train_loader) + 1\n",
    "        print(f\"Epoch[{engine.state.epoch}] Iteration[{iteration}/{len(train_loader)}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "    # Log training and validation results at the end of each epoch\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_epoch_results(engine):\n",
    "        # Training metrics\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        evaluator.run(train_loader)\n",
    "        train_metrics = evaluator.state.metrics\n",
    "        print(f\"Training Results - Epoch: {engine.state.epoch} Avg accuracy: {train_metrics['accuracy']:.2f} Avg loss: {train_metrics['loss']:.2f}\")\n",
    "        \n",
    "        # Validation metrics\n",
    "        evaluator.run(val_loader)\n",
    "        val_metrics = evaluator.state.metrics\n",
    "        print(f\"Validation Results - Epoch: {engine.state.epoch} Avg accuracy: {val_metrics['accuracy']:.2f} Avg loss: {val_metrics['loss']:.2f}\")\n",
    "\n",
    "        model.train()  # Set model back to training mode\n",
    "\n",
    "    # Run the training loop\n",
    "    trainer.run(train_loader, max_epochs=max_epochs)\n",
    "\n",
    "    # Close the logger\n",
    "    tb_logger.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ignite"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/contrib/handlers/tqdm_logger.py:127: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Epoch [1/100]: [50/50] 100%| [00:07<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch[1] Avg accuracy: 0.23 Avg loss: 2.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch[1] Avg accuracy: 0.24 Avg loss: 2.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]: [50/50] 100%| [00:07<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch[2] Avg accuracy: 0.24 Avg loss: 2.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results - Epoch[2] Avg accuracy: 0.25 Avg loss: 2.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]: [50/50] 100%| [00:07<00:00]Engine run is terminating due to exception: \n",
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m pbar \u001b[39m=\u001b[39m ProgressBar()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m pbar\u001b[39m.\u001b[39mattach(trainer)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mrun(train_loader, max_epochs\u001b[39m=\u001b[39;49mepochs)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n\u001b[1;32m    893\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run_generator)\n\u001b[1;32m    936\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:965\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m epoch_time_taken\n\u001b[1;32m    964\u001b[0m handlers_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 965\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fire_event(Events\u001b[39m.\u001b[39;49mEPOCH_COMPLETED)\n\u001b[1;32m    966\u001b[0m epoch_time_taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m handlers_start_time\n\u001b[1;32m    967\u001b[0m \u001b[39m# update time wrt handlers\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:425\u001b[0m, in \u001b[0;36mEngine._fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(event_kwargs)\n\u001b[1;32m    424\u001b[0m first, others \u001b[39m=\u001b[39m ((args[\u001b[39m0\u001b[39m],), args[\u001b[39m1\u001b[39m:]) \u001b[39mif\u001b[39;00m (args \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m) \u001b[39melse\u001b[39;00m ((), args)\n\u001b[0;32m--> 425\u001b[0m func(\u001b[39m*\u001b[39;49mfirst, \u001b[39m*\u001b[39;49m(event_args \u001b[39m+\u001b[39;49m others), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/amir/Documents/clony/SatViT/segmentation.ipynb Cell 26\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m@trainer\u001b[39m\u001b[39m.\u001b[39mon(Events\u001b[39m.\u001b[39mEPOCH_COMPLETED)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_training_results\u001b[39m(trainer):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     train_evaluator\u001b[39m.\u001b[39;49mrun(train_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     metrics \u001b[39m=\u001b[39m train_evaluator\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amir/Documents/clony/SatViT/segmentation.ipynb#X42sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Results - Epoch[\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch\u001b[39m}\u001b[39;00m\u001b[39m] Avg accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Avg loss: \u001b[39m\u001b[39m{\u001b[39;00mmetrics[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n\u001b[1;32m    893\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run_generator)\n\u001b[1;32m    936\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_run_generator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:959\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 959\u001b[0m epoch_time_taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    961\u001b[0m \u001b[39m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:1069\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[1;32m   1068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_function(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbatch)\n\u001b[0;32m-> 1069\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fire_event(Events\u001b[39m.\u001b[39;49mITERATION_COMPLETED)\n\u001b[1;32m   1070\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[1;32m   1072\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m iter_counter \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch_length:\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/engine/engine.py:425\u001b[0m, in \u001b[0;36mEngine._fire_event\u001b[0;34m(self, event_name, *event_args, **event_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(event_kwargs)\n\u001b[1;32m    424\u001b[0m first, others \u001b[39m=\u001b[39m ((args[\u001b[39m0\u001b[39m],), args[\u001b[39m1\u001b[39m:]) \u001b[39mif\u001b[39;00m (args \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m) \u001b[39melse\u001b[39;00m ((), args)\n\u001b[0;32m--> 425\u001b[0m func(\u001b[39m*\u001b[39;49mfirst, \u001b[39m*\u001b[39;49m(event_args \u001b[39m+\u001b[39;49m others), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/metrics/metric.py:310\u001b[0m, in \u001b[0;36mMetric.iteration_completed\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate((tensor_o1, tensor_o2))\n\u001b[1;32m    309\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/satvit/lib/python3.11/site-packages/ignite/metrics/metric.py:607\u001b[0m, in \u001b[0;36mreinit__is_reduced.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    606\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m: Metric, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    608\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_result\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m    609\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/clony/SatViT/utils/accuracy.py:20\u001b[0m, in \u001b[0;36mCustomAccuracy.update\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     18\u001b[0m y_pred, y \u001b[39m=\u001b[39m output\n\u001b[1;32m     19\u001b[0m predicted_classes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_pred, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m correct \u001b[39m=\u001b[39m (predicted_classes \u001b[39m==\u001b[39;49m y)\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_examples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mnumel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall, ConfusionMatrix\n",

    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar\n",
    "from utils.accuracy import CustomAccuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "val_metrics = {\n",
    "    \"accuracy\": CustomAccuracy(),\n",
    "    # \"precision\": Precision(),\n",
    "    # \"recall\": Recall(),\n",
    "    # \"confusion_matrix\": ConfusionMatrix(num_classes=20),\n",
    "    \"loss\": Loss(criterion)\n",
    "}\n",
    "\n",
    "\n",

    "\n",
    "\n",
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch[0].to(device), batch[1].to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    # y_pred = torch.argmax(y_pred, dim=3)\n",
    "    # print(y.shape, y_pred.shape)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "\n",
    "\n",
    "\n",
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        # y = F.one_hot(y, num_classes=20)\n",
    "        y_pred = model(x)\n",
    "        # y_pred = F.one_hot(torch.argmax(model(x), dim=3), num_classes=20).astype(int)\n",
    "        # y_pred = y_pred.permute(0, 3, 1, 2)\n",
    "        # print(y.shape, y_pred.shape)\n",
    "        return y_pred, y\n",
    "\n",
    "\n",
    "train_evaluator = Engine(validation_step)\n",
    "val_evaluator = Engine(validation_step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Attach metrics to the evaluators\n",
    "for name, metric in val_metrics.items():\n",
    "    metric.attach(train_evaluator, name)\n",
    "\n",
    "for name, metric in val_metrics.items():\n",

    "    metric.attach(val_evaluator, name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# How many batches to wait before logging training status\n",
    "log_interval = 100\n",
    "\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",

    "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Score function to return current value of any metric we defined above in val_metrics\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "# Checkpoint to store n_saved best models wrt score function\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer), # helps fetch the trainer's state\n",
    ")\n",
    "  \n",
    "# Save the model after every epoch of val_evaluator is completed\n",

    "val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")\n",
    "\n",
    "# Attach handler to plot trainer's loss every 100 iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.EPOCH_COMPLETED(every=100),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",

    "    )\n",
    "\n",
    "\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer)\n",
    "\n",
    "trainer.run(train_loader, max_epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(val_loader))\n",
    "model.eval()\n",
    "output = torch.argmax(model(img.to(device)), dim=3)\n",
    "\n",
    "fix, axes = plt.subplots(1,3, figsize=(10,10))\n",
    "axes[0].imshow(get_rgb(img[0][:,:-1,:,:].numpy()))\n",
    "axes[1].imshow(label[0].numpy(), cmap='inferno')\n",
    "axes[2].imshow(output[0].cpu().numpy(), cmap='inferno')\n",
    "\n",
    "axes[0].set_title('img')\n",
    "axes[1].set_title(f'Label: {label[0].unique().tolist()}')\n",
    "axes[2].set_title(f'Prediction: {output[0].cpu().unique().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "checkpoint = torch.load('weights/epoch_80.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
